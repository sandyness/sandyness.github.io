<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Transformer on ä¸€ç‰‡ç”Ÿèœå¶</title>
        <link>https://sandyness.github.io/tags/transformer/</link>
        <description>Recent content in Transformer on ä¸€ç‰‡ç”Ÿèœå¶</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 07 Aug 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://sandyness.github.io/tags/transformer/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Transformerè®ºæ–‡é˜…è¯»</title>
        <link>https://sandyness.github.io/p/transformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</link>
        <pubDate>Sun, 07 Aug 2022 00:00:00 +0000</pubDate>
        
        <guid>https://sandyness.github.io/p/transformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</guid>
        <description>&lt;p&gt;éš¨è‘—æ·±åº¦å­¸ç¿’çš„ç™¼å±•ï¼Œè‡ªç„¶èªè¨€è™•ç† (Natural Language Processingï¼ŒNLP) æŠ€è¡“è¶Šè¶¨æˆç†Ÿï¼Œè¿‘å¹´ä¾†åœ¨æƒ…æ„Ÿåˆ†æã€æ©Ÿå™¨ç¿»è­¯ã€èªéŸ³è¾¨è­˜ã€å°è©±æ©Ÿå™¨äººç­‰ä»»å‹™ä¸Šå‡æœ‰å¾ˆä¸éŒ¯çš„çµæœã€‚&lt;/p&gt;
&lt;p&gt;è‡ªç„¶èªè¨€è™•ç†æ˜¯æŒ‡è®“é›»è…¦èƒ½å¤ åˆ†æã€ç†è§£äººé¡èªè¨€çš„ä¸€é …æŠ€è¡“ï¼Œè€Œäººé¡èªè¨€å…·æœ‰å‰å¾Œé †åºã€ä¸Šä¸‹æ–‡é—œä¿‚ï¼Œå°æ–¼é€™ç¨®æ™‚é–“åºåˆ—çš„è³‡æ–™å¾ˆå¸¸ä½¿ç”¨å¾ªç’°ç¥ç¶“ç¶²è·¯ (Recurrent Neural Networkï¼ŒRNN)ã€‚ä½†ç”±æ–¼ RNN é›£ä»¥é€²è¡Œå¹³è¡Œé‹ç®—ï¼Œå› æ­¤ Google æå‡ºäº†ä¸€ç¨®ä¸ä½¿ç”¨ RNNã€CNNï¼Œåƒ…ä½¿ç”¨è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶ (self-attention mechanism) çš„ç¶²è·¯æ¶æ§‹ â€” Transformerã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡å°‡è¦ä»‹ç´¹ Attention Is All You Need è«–æ–‡ï¼Œç™¼è¡¨æ–¼ NIPS 2017ã€‚å…¶ç¶²è·¯æ¶æ§‹æ˜¯åŸºæ–¼ Seq2Seq + self-attention mechanismã€‚åœ¨é–‹å§‹é–±è®€ä¹‹å‰ï¼Œå»ºè­°å…ˆäº†è§£ RNNã€Seq2Seq åŠæ³¨æ„åŠ›æ©Ÿåˆ¶ (attention mechanism)ï¼Œæœ¬æ–‡åƒ…å¤§ç•¥åœ°ä»‹ç´¹ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://sandyness.github.io/content/post/transformer.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;è¿™æ˜¯å›¾ç‰‡&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;seq2seq&#34;&gt;Seq2Seq&lt;/h2&gt;
&lt;p&gt;Seq2Seqä¸»è¦ç”±å…©ç¯‡è«–æ–‡æå‡ºï¼šSequence to Sequence Learning with Neural Networksã€Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translationï¼Œå…©è€…æ¦‚å¿µç›¸åŒï¼Œå·®åˆ¥åœ¨æ–¼ä½¿ç”¨ä¸åŒçš„ RNN æ¨¡å‹ã€‚å‰è€…ä½¿ç”¨ LSTMï¼Œè€Œå¾Œè€…ä½¿ç”¨ GRUã€‚&lt;/p&gt;
&lt;p&gt;å…¶æ¶æ§‹ç‚º Encoder-Decoderï¼Œå¦‚ä¸‹åœ–æ‰€ç¤ºï¼ŒEncoder æœƒå…ˆå°‡è¼¸å…¥å¥å­é€²è¡Œç·¨ç¢¼ï¼Œå¾—åˆ°çš„ç‹€æ…‹æœƒå‚³çµ¦ Decoder è§£ç¢¼ç”Ÿæˆç›®æ¨™å¥å­ã€‚&lt;/p&gt;
&lt;h2 id=&#34;æ³¨æ„åŠ›æœºåˆ¶&#34;&gt;æ³¨æ„åŠ›æœºåˆ¶&lt;/h2&gt;
&lt;p&gt;ä½†æ˜¯ç•¶è¨Šæ¯å¤ªé•·æ™‚ï¼Œseq2seq å®¹æ˜“ä¸Ÿå¤±è¨Šæ¯ï¼Œå› æ­¤å¼•å…¥äº†æ³¨æ„åŠ›æ©Ÿåˆ¶ (Attention Mechanism)ã€‚å…¶æ¦‚å¿µç‚ºå°‡ Encoder æ‰€æœ‰è³‡è¨Šéƒ½å‚³çµ¦ Decoderï¼Œè®“ Decoder æ±ºå®šæŠŠæ³¨æ„åŠ›æ”¾åœ¨å“ªäº›è³‡è¨Šä¸Šã€‚æœ‰å…©ç¯‡ç¶“å…¸çš„ä»£è¡¨è«–æ–‡ï¼šNeural Machine Translation by Jointly Learning to Align and Translateã€Effective Approaches to Attention-based Neural Machine Translationã€‚&lt;/p&gt;
&lt;h2 id=&#34;ç¼–ç å™¨&#34;&gt;ç¼–ç å™¨&lt;/h2&gt;
&lt;p&gt;å…ˆä¾†çœ‹å·¦é‚Šçš„ Encoder éƒ¨åˆ†ï¼Œinput å…ˆç¶“é embedding å±¤è½‰æ›ç‚ºä¸€å€‹å‘é‡ï¼Œç„¶å¾Œåœ¨é€²å…¥ layer å‰æœƒå…ˆèˆ‡ Positional Encoding ç›¸åŠ  (Input Embedding è·Ÿ Positional Encoding çš„ç¶­åº¦ç›¸ç­‰)ï¼Œé€™å€‹ Positional Encoding å°±æ˜¯è©èªçš„ä½ç½®ç·¨ç¢¼ï¼Œç›®çš„æ˜¯ç‚ºäº†è®“æ¨¡å‹è€ƒæ…®è©èªä¹‹é–“çš„é †åºã€‚&lt;/p&gt;
&lt;p&gt;è«–æ–‡ä½¿ç”¨ sin, cos å‡½æ•¸é€²è¡Œä½ç½®ç·¨ç¢¼ï¼Œå…¬å¼å¦‚ä¸‹ï¼Œå…¶ä¸­ pos ç‚ºè©èªåœ¨åºåˆ—ä¸­çš„ä½ç½®ã€2i, 2i+1 ç‚ºè©²è©èªåœ¨ Positional Encoding ç¶­åº¦ä¸Šçš„ indexã€d_model ç‚º Positional Encoding çš„ç¶­åº¦ (èˆ‡ Input Embedding ç¶­åº¦ç›¸ç­‰)ï¼Œé è¨­å€¼ç‚º 512ã€‚&lt;/p&gt;
&lt;p&gt;é€™æ¨£è¬›å¯èƒ½è¦ºå¾—æœ‰é»é›£ä»¥ç†è§£ï¼Œä¾†èˆ‰å€‹ä¾‹å­ï¼Œå‡è¨­è¦è¨ˆç®—åºåˆ—ä¸­çš„ç¬¬äºŒå€‹è©èªï¼Œæ­¤æ™‚ pos=1ï¼Œå‰‡ Positional Encoding (PE) ç‚ºä»¥ä¸‹æ¨£å­&lt;/p&gt;
&lt;p&gt;â“ ç‚ºä»€éº¼è¦ä½¿ç”¨ sin, cos å‡½æ•¸é€²è¡Œç·¨ç¢¼?&lt;/p&gt;
&lt;p&gt;å› ç‚º sin, cos å‡½æ•¸å¯ä»¥è¡¨ç¤ºç‚ºå…©å€‹å‘é‡é–“çš„ç·šæ€§é—œä¿‚ï¼Œèƒ½å¤ å‘ˆç¾ä¸åŒè©èªä¹‹é–“çš„ç›¸å°ä½ç½®ï¼Œä¸¦ä¸”ä¸å—é™åºåˆ—é•·åº¦çš„é™åˆ¶ï¼Œæ¯”è¼ƒä¸æœƒæœ‰é‡è¤‡çš„å•é¡Œã€‚&lt;/p&gt;
&lt;p&gt;èµ„æº
æ­¤å¤–ï¼Œsin, cos å‡½æ•¸æœ‰ä¸Šä¸‹ç•Œ (è½æ–¼ [0, 1] ä¹‹é–“)ã€ç©©å®šå¾ªç’°çš„æ€§è³ªã€‚&lt;/p&gt;
&lt;p&gt;èµ„æº
â“ ç‚ºä»€éº¼æ˜¯èˆ‡ Positional Encoding ç›¸åŠ ï¼Œè€Œä¸æ˜¯ concat ?&lt;/p&gt;
&lt;p&gt;å› ç‚ºå…¶å¯¦åšç›¸åŠ å¾—åˆ°çš„çµæœèˆ‡ concat æ˜¯ç›¸åŒçš„ã€‚å‡è¨­æœ‰ä¸€è¼¸å…¥åºåˆ— xiï¼Œå…¶ä½ç½®ä½¿ç”¨ç°¡å–®çš„ one-hot encoding è¡¨ç¤ºç‚º pi=(0, â€¦, 1, 0)ï¼ŒW_x, W_p ç‚ºå…¶ç›¸å°æ‡‰çš„æ¬Šé‡ã€‚&lt;/p&gt;
&lt;p&gt;å°‡ W_x, W_p åˆä½µç‚º Wï¼Œxi, pi åˆä½µç‚º Xï¼Œè€Œ W èˆ‡ X çš„ Inner Product å¯ç¶“ç”±ç·šæ€§ä»£æ•¸çš„æ€§è³ªï¼Œæ‹†åˆ†ç‚º W_x è·Ÿ xi çš„ Inner Product + W_p è·Ÿ pi çš„ Inner Productã€‚å› æ­¤å¯ä»¥å¾—çŸ¥ Input Embedding èˆ‡ Positional Encoding ç›¸åŠ æ‰€å¾—åˆ°çš„çµæœè·Ÿå…©è€… concat æ˜¯ä¸€æ¨£çš„ã€‚&lt;/p&gt;
&lt;p&gt;å¤šå¤´æ³¨æ„åŠ›&lt;/p&gt;
&lt;p&gt;Input Embedding èˆ‡ Positional Encoding ç›¸åŠ å¾Œæœƒé€²å…¥åˆ° layer è£¡ï¼ŒEncoder æœ‰å…©å€‹å­å±¤ Multi-head Attentionã€Feed Forwardã€‚åœ¨ä»‹ç´¹ Multi-head Attention ä¹‹å‰ï¼Œå…ˆä¾†èªªæ˜è‡ªæ³¨æ„æ©Ÿåˆ¶ (self-attention mechanism)&lt;/p&gt;
&lt;p&gt;è‡ªæ³¨æ„æœºåˆ¶
è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶æœ‰ä¸‰å€‹é‡è¦çš„åƒæ•¸ q, k, vï¼Œè€Œé€™äº›åƒæ•¸æ˜¯ç”± input xi ç¶“é embedding å±¤è½‰æ›ç‚º aiï¼Œæ¥è‘— ai é€²å…¥åˆ° self-attention layer æœƒä¹˜ä¸Šä¸‰å€‹ä¸åŒçš„ matrix æ‰€å¾—åˆ°çš„ã€‚&lt;/p&gt;
&lt;p&gt;q (query): æ˜¯æŒ‡ç•¶å‰çš„è©å‘é‡ï¼Œç”¨æ–¼å°æ¯å€‹ key åšåŒ¹é…ç¨‹åº¦çš„æ‰“åˆ†&lt;/p&gt;
&lt;p&gt;k (key): æ˜¯æŒ‡åºåˆ—ä¸­çš„æ‰€æœ‰è©å‘é‡&lt;/p&gt;
&lt;p&gt;v (value): æ˜¯æŒ‡å¯¦éš›çš„åºåˆ—å…§å®¹&lt;/p&gt;
&lt;p&gt;ç”±ä¸‹åœ–å¯ä»¥çœ‹åˆ° q1 æœƒå°æ¯ä¸€å€‹ k åš Inner Product å¾—åˆ° q, k ä¹‹é–“åŒ¹é…çš„ç›¸ä¼¼ç¨‹åº¦ Î±1, 1ã€Î±1, 2ã€â€¦ï¼Œç„¶å¾Œåšä¸€ç³»åˆ—çš„é‹ç®—å¾—åˆ°è¼¸å‡ºï¼Œé€™äº›è¨ˆç®—æ­¥é©Ÿç¨±ç‚º Scaled Dot-Product Attention&lt;/p&gt;
&lt;p&gt;èµ„æº
ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
æ¥è‘—ä¾†çœ‹æ•¸å­¸å¼å­æœƒæ›´äº†è§£ï¼Œç”±ä¸‹åˆ—å…¬å¼å¾—çŸ¥ q, k æœƒå…ˆåš Inner Productï¼Œå¾—åˆ°çš„å€¼æ˜¯åŒ¹é…çš„ç›¸ä¼¼ç¨‹åº¦ï¼Œé™¤ä»¥ sqrt(dk) å¾Œï¼Œå†åš softmax è¨ˆç®—å‡º v çš„æ¬Šé‡ï¼Œæœ€å¾Œå°‡è©²æ¬Šé‡èˆ‡ v åšåŠ æ¬Šé‹ç®—ï¼Œå…¶ä¸­ q, k ç¶­åº¦éƒ½ç‚º dkï¼Œv ç¶­åº¦ç‚º dvã€‚&lt;/p&gt;
&lt;p&gt;çœ‹åˆ°é€™è£¡å¯èƒ½æœƒç–‘æƒ‘ç‚ºä»€éº¼è¦é™¤ä»¥ sqrt(dk)? ä¹‹æ‰€ä»¥é€™æ¨£åšçš„åŸå› æ˜¯ç‚ºäº†é¿å…ç•¶ dk (q, k çš„ç¶­åº¦) å¤ªå¤§æ™‚ï¼Œq, k Inner Product çš„å€¼éå¤§ï¼Œsoftmax è¨ˆç®—çš„å€¼è½å…¥é£½å’Œå€è€Œå°è‡´æ¢¯åº¦ä¸ç©©å®šã€‚&lt;/p&gt;
&lt;p&gt;å¤šå¤´æ³¨æ„åŠ›
çµ‚æ–¼è¦ä¾†ä»‹ç´¹ Multi-Head Attention å•¦~ å…¶é‹ç®—æ–¹å¼èˆ‡ self-attention mechanism ç›¸åŒï¼Œå·®ç•°åœ¨æ–¼æœƒå…ˆå°‡ q, k, v æ‹†åˆ†æˆå¤šå€‹ä½ç¶­åº¦çš„å‘é‡ï¼Œç”±ä¸‹åœ–å¯çœ‹åˆ°è‹¥å‡è¨­ head=2ï¼Œqi æœƒæ‹†åˆ†æˆ qi,1ã€qi,2ï¼Œæ¥è‘—ç¹¼çºŒè·Ÿä¸Šè¿°ä¸€æ¨£çš„æ­¥é©Ÿï¼Œæœ€å¾Œå†æŠŠé€™äº› head è¼¸å‡º concat èµ·ä¾†åšä¸€æ¬¡ç·šæ€§è¨ˆç®—ã€‚&lt;/p&gt;
&lt;p&gt;é€™æ¨£çš„å¥½è™•æ˜¯èƒ½å¤ è®“å„å€‹ head (q, k, v) é—œæ³¨ä¸åŒçš„è³‡è¨Šï¼Œæœ‰äº›é—œæ³¨ localã€æœ‰äº›é—œæ³¨ global è³‡è¨Šç­‰ã€‚&lt;/p&gt;
&lt;p&gt;èµ„æº
ç”±ä¸‹åœ–å¯çœ‹åˆ° q, k, v æœƒåš h æ¬¡çš„ç·šæ€§æ˜ å°„åˆ°ä½ç¶­åº¦çš„å‘é‡ï¼Œå†é€²è¡Œ Scaled Dot-Product Attentionï¼Œæœ€å¾Œå°‡å…¶ concatã€linear å¾—åˆ°è¼¸å‡ºã€‚&lt;/p&gt;
&lt;p&gt;ä»¥ä¸‹æ˜¯ Multi-Head å…¬å¼ï¼Œå…¶ä¸­ h=8ã€dk=dv=d_model/h&lt;/p&gt;
&lt;p&gt;æ·»åŠ å’Œè§„èŒƒ&lt;/p&gt;
&lt;p&gt;ç¶“é Multi-head Attention å¾Œæœƒé€²å…¥ Add &amp;amp; Norm å±¤ï¼Œé€™ä¸€å±¤æ˜¯æŒ‡ residual connection åŠ layer normalizationã€‚å‰ä¸€å±¤çš„è¼¸å‡º Sublayer æœƒèˆ‡åŸè¼¸å…¥ x ç›¸åŠ  (residual connection)ï¼Œä»¥æ¸›ç·©æ¢¯åº¦æ¶ˆå¤±çš„å•é¡Œï¼Œç„¶å¾Œå†åš layer normalizationã€‚&lt;/p&gt;
&lt;p&gt;ğŸ“š å±‚å½’ä¸€åŒ–ä¸æ‰¹é‡å½’ä¸€åŒ–&lt;/p&gt;
&lt;p&gt;åœ¨ä¹‹å‰çš„æ–‡ç« æœ‰ä»‹ç´¹é Batch Normalization (BN)ï¼Œå…¶ä½œæ³•æ˜¯åœ¨æ¯ä¸€å€‹ mini-batch çš„ input feature åš normalizeï¼Œé€™æ¨£çš„æ–¹å¼é›–ç„¶åœ¨ CNN ä¸Šç²å¾—äº†å¾ˆå¥½çš„æ•ˆæœï¼Œä½†ä»ç„¶å­˜åœ¨ä¸€äº›ç¼ºé»ï¼š&lt;/p&gt;
&lt;p&gt;éæ–¼ä¾è³´ batch sizeï¼Œå¦‚æœ batch size å¤ªå°ï¼ŒBN çš„æ•ˆæœæœƒæ˜é¡¯ä¸‹é™ã€‚
ä¸å¤ªé©ç”¨æ–¼æ™‚é–“åºåˆ—ï¼Œå› ç‚ºæ–‡æœ¬åºåˆ—çš„é•·åº¦é€šå¸¸ä¸ä¸€è‡´ï¼Œå¼·åˆ¶å°æ¯å€‹æ–‡æœ¬åŸ·è¡Œ BN ä¸å¤§åˆç†ã€‚
å› æ­¤åœ¨ RNN ä¸­è¼ƒå¸¸ä½¿ç”¨ Layer Normalization (LN)ï¼Œæ¦‚å¿µèˆ‡ BN é¡ä¼¼ï¼Œå·®åˆ¥åœ¨æ–¼ LN æ˜¯å°æ¯ä¸€å€‹æ¨£æœ¬é€²è¡Œ normalizeã€‚ç”±ä¸‹åœ–å¯ä»¥å¾ˆæ¸…æ¥šçš„çœ‹å‡ºå…©è€…çš„å·®ç•°ï¼Œå…¶ä¸­æ¯ä¸€è¡Œæ˜¯æŒ‡æ¨£æœ¬ï¼Œæ¯ä¸€åˆ—æ˜¯æ¨£æœ¬ç‰¹å¾µã€‚&lt;/p&gt;
&lt;p&gt;èµ„æº
ä½ç½®å‰é¦ˆç½‘ç»œï¼ˆå‰é¦ˆï¼‰&lt;/p&gt;
&lt;p&gt;æ¥è‘—é€²å…¥åˆ° FFN å±¤ï¼Œç”±ä¸‹åˆ—å…¬å¼å¯ä»¥çœ‹åˆ°è¼¸å…¥ x å…ˆåšç·šæ€§é‹ç®—å¾Œï¼Œé€å…¥ ReLUï¼Œå†åšä¸€æ¬¡ç·šæ€§é‹ç®—ã€‚å…¶ä¸­è¼¸å…¥è¼¸å‡ºçš„ç¶­åº¦ d_model=512ï¼Œè€Œä¸­é–“å±¤çš„ç¶­åº¦ dff = 2048&lt;/p&gt;
&lt;h2 id=&#34;è§£ç å™¨&#34;&gt;è§£ç å™¨&lt;/h2&gt;
&lt;p&gt;çœ‹åˆ°é€™è£¡å·²ç¶“ç†è§£ Encoder çš„é‹ç®—éç¨‹äº†ï¼Œå†ä¾†çœ‹å³é‚Š Decoder çš„éƒ¨åˆ†å§ï¼&lt;/p&gt;
&lt;p&gt;Decoder èˆ‡ Encoder ä¸€æ¨£æœƒå…ˆè·Ÿ Positional Encoding ç›¸åŠ å†é€²å…¥ layerï¼Œä¸åŒçš„æ˜¯ Decoder æœ‰ä¸‰å€‹å­å±¤ Masked Multi-head Attentionã€Multi-head Attentionã€Feed Forwardã€‚æ­¤å¤–ï¼Œä¸­é–“å±¤ Multi-head Attention çš„è¼¸å…¥ q ä¾†è‡ªæ–¼æœ¬èº«å‰ä¸€å±¤çš„è¼¸å‡ºï¼Œè€Œ k, v å‰‡æ˜¯ä¾†è‡ªæ–¼ Encoder çš„è¼¸å‡ºã€‚&lt;/p&gt;
&lt;p&gt;è’™é¢å¤šå¤´æ³¨æ„åŠ›&lt;/p&gt;
&lt;p&gt;ç”±æ–¼å…¶ä»–å…©å±¤è·Ÿ Encoder å¤§è‡´ç›¸åŒï¼Œæ‰€ä»¥å°±è·³éä¾†ä»‹ç´¹ Decoder ä¸­æ‰æœ‰çš„ Masked Multi-head Attentionã€‚&lt;/p&gt;
&lt;p&gt;Transformer çš„ Mask æ©Ÿåˆ¶æœ‰å…©ç¨®ï¼šPadding Maskã€Sequence Mask&lt;/p&gt;
&lt;p&gt;Padding Mask åœ¨ Encoderå’Œ Decoder ä¸­éƒ½æœ‰ä½¿ç”¨åˆ°ï¼Œç›®çš„æ˜¯ç‚ºäº†é™åˆ¶æ¯å€‹è¼¸å…¥çš„é•·åº¦è¦ç›¸åŒï¼Œå°æ–¼è¼ƒçŸ­çš„å¥å­æœƒå°‡ä¸è¶³çš„éƒ¨åˆ†è£œ 0
Sequence Mask åªç”¨æ–¼ Decoderï¼Œç›®çš„æ˜¯ç‚ºäº†é˜²æ­¢æ¨¡å‹çœ‹åˆ°æœªä¾†çš„è³‡è¨Šï¼Œå› æ­¤åœ¨è¶…éç•¶å‰æ™‚åˆ» t çš„è¼¸å‡ºæœƒåŠ ä¸Šä¸€å€‹ maskï¼Œç¢ºä¿æ¨¡å‹çš„é æ¸¬åªä¾è³´å°æ–¼ç•¶å‰æ™‚åˆ»çš„è¼¸å‡ºã€‚
Sequence Mask çš„åšæ³•æ˜¯é€šéä¸€å€‹ä¸Šä¸‰è§’çŸ©é™£ä¾†å¯¦ç¾ï¼Œå°‡é€™äº›å€åŸŸçš„å€¼éƒ½è¨­å®šç‚ºè² ç„¡çª®ï¼Œå¦‚æ­¤ä¸€ä¾†é€™äº›å…ƒç´ ç¶“é softmax å¾Œéƒ½æœƒè®Šç‚º 0 ä»¥é”åˆ° mask çš„æ•ˆæœã€‚&lt;/p&gt;
&lt;h2 id=&#34;é“¾æ¥&#34;&gt;é“¾æ¥&lt;/h2&gt;
&lt;p&gt;ğŸ“ &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1706.03762&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Transformerè®ºæ–‡&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
