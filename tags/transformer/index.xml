<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Transformer on ä¸€ç‰‡ç”Ÿèœå¶</title>
        <link>https://sandyness.github.io/tags/transformer/</link>
        <description>Recent content in Transformer on ä¸€ç‰‡ç”Ÿèœå¶</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 07 Aug 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://sandyness.github.io/tags/transformer/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Transformerè®ºæ–‡é˜…è¯»</title>
        <link>https://sandyness.github.io/p/transformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</link>
        <pubDate>Sun, 07 Aug 2022 00:00:00 +0000</pubDate>
        
        <guid>https://sandyness.github.io/p/transformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</guid>
        <description>&lt;p&gt;éšç€æ·±åº¦å­¦ä¹ çš„å‘å±•ï¼Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æŠ€æœ¯è¶Šæ¥è¶Šæˆç†Ÿã€‚åœ¨æœºå™¨ç¿»è¯‘ã€è¯­éŸ³è¯†åˆ«ã€æƒ…æ„Ÿåˆ†æã€æ–‡æœ¬ç”Ÿæˆã€äººæœºå¯¹è¯ç­‰ä»»åŠ¡ä¸Šéƒ½æœ‰å¾ˆå¥½çš„ç»“æœã€‚ä½†ç”±äºRNNéš¾ä»¥è¿›è¡Œå¹¶è¡Œè®¡ç®—ï¼Œ
å› æ­¤Googleæå‡ºäº†ä¸€ç§ä¸ä½¿ç”¨RNNå’ŒCNN,è€Œæ˜¯ä½¿ç”¨å«&lt;strong&gt;è‡ªæ³¨æ„åŠ›æœºåˆ¶(self-attention mechanism)-Transformer&lt;/strong&gt;ã€‚å…¶ç½‘ç»œæ¶æ„æ˜¯åŸºäº Seq2Seq + self-attention mechanismã€‚&lt;/p&gt;
&lt;h2 id=&#34;seq2seq&#34;&gt;Seq2Seq&lt;/h2&gt;
&lt;p&gt;Seq2Seqä¸»è¦ç”±ä¸¤ç¯‡è®ºæ–‡æå‡ºï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1409.3215&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Sequence to Sequence Learning with Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1406.1078&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ä¸¤è€…å·®åˆ«æ˜¯å‰è€…ä½¿ç”¨LSTMï¼Œåè€…ä½¿ç”¨GRUã€‚&lt;/p&gt;
&lt;h2 id=&#34;æ³¨æ„åŠ›æœºåˆ¶&#34;&gt;æ³¨æ„åŠ›æœºåˆ¶&lt;/h2&gt;
&lt;p&gt;ä½†æ˜¯å½“è®¯æ¯å¤ªé•¿æ—¶ï¼Œseq2seqå®¹æ˜“ä¸¢å¤±è®¯æ¯ï¼Œå› æ­¤å¼•å…¥äº†æ³¨æ„åŠ›æœºåˆ¶ã€‚å…¶æ¦‚å¿µä¸ºå°†Encoderæ‰€æœ‰èµ„è®¯ä¼ ç»™Decoderï¼Œè®©Decoderå†³å®šå°†æ³¨æ„åŠ›æ”¾åœ¨å“ªäº›èµ„è®¯ä¸Šã€‚
æœ‰ä¸¤ç¯‡è®ºæ–‡ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Neural Machine Translation by Jointly Learning to Align and Translate&lt;/li&gt;
&lt;li&gt;Effective Approaches to Attention-based Neural Machine Translation&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ç¼–ç å™¨&#34;&gt;ç¼–ç å™¨&lt;/h2&gt;
&lt;p&gt;Encoderéƒ¨åˆ†ï¼Œinput å…ˆç¶“é embedding å±¤è½‰æ›ç‚ºä¸€å€‹å‘é‡ï¼Œç„¶å¾Œåœ¨é€²å…¥ layer å‰æœƒå…ˆèˆ‡ Positional Encoding ç›¸åŠ  (Input Embedding è·Ÿ Positional Encoding çš„ç¶­åº¦ç›¸ç­‰)ï¼Œé€™å€‹ Positional Encoding å°±æ˜¯è©èªçš„ä½ç½®ç·¨ç¢¼ï¼Œç›®çš„æ˜¯ç‚ºäº†è®“æ¨¡å‹è€ƒæ…®è©èªä¹‹é–“çš„é †åºã€‚&lt;/p&gt;
&lt;p&gt;è«–æ–‡ä½¿ç”¨ sin, cos å‡½æ•¸é€²è¡Œä½ç½®ç·¨ç¢¼ï¼Œå…¬å¼å¦‚ä¸‹ï¼Œå…¶ä¸­ pos ç‚ºè©èªåœ¨åºåˆ—ä¸­çš„ä½ç½®ã€2i, 2i+1 ç‚ºè©²è©èªåœ¨ Positional Encoding ç¶­åº¦ä¸Šçš„ indexã€d_model ç‚º Positional Encoding çš„ç¶­åº¦ (èˆ‡ Input Embedding ç¶­åº¦ç›¸ç­‰)ï¼Œé è¨­å€¼ç‚º 512ã€‚&lt;/p&gt;
&lt;p&gt;é€™æ¨£è¬›å¯èƒ½è¦ºå¾—æœ‰é»é›£ä»¥ç†è§£ï¼Œä¾†èˆ‰å€‹ä¾‹å­ï¼Œå‡è¨­è¦è¨ˆç®—åºåˆ—ä¸­çš„ç¬¬äºŒå€‹è©èªï¼Œæ­¤æ™‚ pos=1ï¼Œå‰‡ Positional Encoding (PE) ç‚ºä»¥ä¸‹æ¨£å­&lt;/p&gt;
&lt;p&gt;â“ ç‚ºä»€éº¼è¦ä½¿ç”¨ sin, cos å‡½æ•¸é€²è¡Œç·¨ç¢¼?&lt;/p&gt;
&lt;p&gt;å› ç‚º sin, cos å‡½æ•¸å¯ä»¥è¡¨ç¤ºç‚ºå…©å€‹å‘é‡é–“çš„ç·šæ€§é—œä¿‚ï¼Œèƒ½å¤ å‘ˆç¾ä¸åŒè©èªä¹‹é–“çš„ç›¸å°ä½ç½®ï¼Œä¸¦ä¸”ä¸å—é™åºåˆ—é•·åº¦çš„é™åˆ¶ï¼Œæ¯”è¼ƒä¸æœƒæœ‰é‡è¤‡çš„å•é¡Œã€‚&lt;/p&gt;
&lt;p&gt;æ­¤å¤–ï¼Œsin, cos å‡½æ•¸æœ‰ä¸Šä¸‹ç•Œ (è½æ–¼ [0, 1] ä¹‹é–“)ã€ç©©å®šå¾ªç’°çš„æ€§è³ªã€‚&lt;/p&gt;
&lt;p&gt;â“ ç‚ºä»€éº¼æ˜¯èˆ‡ Positional Encoding ç›¸åŠ ï¼Œè€Œä¸æ˜¯ concat ?&lt;/p&gt;
&lt;p&gt;å› ç‚ºå…¶å¯¦åšç›¸åŠ å¾—åˆ°çš„çµæœèˆ‡ concat æ˜¯ç›¸åŒçš„ã€‚å‡è¨­æœ‰ä¸€è¼¸å…¥åºåˆ— xiï¼Œå…¶ä½ç½®ä½¿ç”¨ç°¡å–®çš„ one-hot encoding è¡¨ç¤ºç‚º pi=(0, â€¦, 1, 0)ï¼ŒW_x, W_p ç‚ºå…¶ç›¸å°æ‡‰çš„æ¬Šé‡ã€‚&lt;/p&gt;
&lt;p&gt;å°‡ W_x, W_p åˆä½µç‚º Wï¼Œxi, pi åˆä½µç‚º Xï¼Œè€Œ W èˆ‡ X çš„ Inner Product å¯ç¶“ç”±ç·šæ€§ä»£æ•¸çš„æ€§è³ªï¼Œæ‹†åˆ†ç‚º W_x è·Ÿ xi çš„ Inner Product + W_p è·Ÿ pi çš„ Inner Productã€‚å› æ­¤å¯ä»¥å¾—çŸ¥ Input Embedding èˆ‡ Positional Encoding ç›¸åŠ æ‰€å¾—åˆ°çš„çµæœè·Ÿå…©è€… concat æ˜¯ä¸€æ¨£çš„ã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;å¤šå¤´æ³¨æ„åŠ›&lt;/p&gt;
&lt;p&gt;Input Embedding èˆ‡ Positional Encoding ç›¸åŠ å¾Œæœƒé€²å…¥åˆ° layer è£¡ï¼ŒEncoder æœ‰å…©å€‹å­å±¤ Multi-head Attentionã€Feed Forwardã€‚åœ¨ä»‹ç´¹ Multi-head Attention ä¹‹å‰ï¼Œå…ˆä¾†èªªæ˜è‡ªæ³¨æ„æ©Ÿåˆ¶ (self-attention mechanism)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;è‡ªæ³¨æ„æœºåˆ¶&lt;/p&gt;
&lt;p&gt;è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶æœ‰ä¸‰å€‹é‡è¦çš„åƒæ•¸ q, k, vï¼Œè€Œé€™äº›åƒæ•¸æ˜¯ç”± input xi ç¶“é embedding å±¤è½‰æ›ç‚º aiï¼Œæ¥è‘— ai é€²å…¥åˆ° self-attention layer æœƒä¹˜ä¸Šä¸‰å€‹ä¸åŒçš„ matrix æ‰€å¾—åˆ°çš„ã€‚&lt;/p&gt;
&lt;p&gt;q (query): æ˜¯æŒ‡ç•¶å‰çš„è©å‘é‡ï¼Œç”¨æ–¼å°æ¯å€‹ key åšåŒ¹é…ç¨‹åº¦çš„æ‰“åˆ†&lt;/p&gt;
&lt;p&gt;k (key): æ˜¯æŒ‡åºåˆ—ä¸­çš„æ‰€æœ‰è©å‘é‡&lt;/p&gt;
&lt;p&gt;v (value): æ˜¯æŒ‡å¯¦éš›çš„åºåˆ—å…§å®¹&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;å¤šå¤´æ³¨æ„åŠ›&lt;/p&gt;
&lt;p&gt;Multi-Head Attention å…¶é‹ç®—æ–¹å¼èˆ‡ self-attention mechanism ç›¸åŒï¼Œå·®ç•°åœ¨æ–¼æœƒå…ˆå°‡ q, k, v æ‹†åˆ†æˆå¤šå€‹ä½ç¶­åº¦çš„å‘é‡ï¼Œç”±ä¸‹åœ–å¯çœ‹åˆ°è‹¥å‡è¨­ head=2ï¼Œqi æœƒæ‹†åˆ†æˆ qi,1ã€qi,2ï¼Œæ¥è‘—ç¹¼çºŒè·Ÿä¸Šè¿°ä¸€æ¨£çš„æ­¥é©Ÿï¼Œæœ€å¾Œå†æŠŠé€™äº› head è¼¸å‡º concat èµ·ä¾†åšä¸€æ¬¡ç·šæ€§è¨ˆç®—ã€‚é€™æ¨£çš„å¥½è™•æ˜¯èƒ½å¤ è®“å„å€‹ head (q, k, v) é—œæ³¨ä¸åŒçš„è³‡è¨Šï¼Œæœ‰äº›é—œæ³¨ localã€æœ‰äº›é—œæ³¨ global è³‡è¨Šç­‰ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æ·»åŠ å’Œè§„èŒƒ&lt;/p&gt;
&lt;p&gt;ç¶“é Multi-head Attention å¾Œæœƒé€²å…¥ Add &amp;amp; Norm å±¤ï¼Œé€™ä¸€å±¤æ˜¯æŒ‡ residual connection åŠ layer normalizationã€‚å‰ä¸€å±¤çš„è¼¸å‡º Sublayer æœƒèˆ‡åŸè¼¸å…¥ x ç›¸åŠ  (residual connection)ï¼Œä»¥æ¸›ç·©æ¢¯åº¦æ¶ˆå¤±çš„å•é¡Œï¼Œç„¶å¾Œå†åš layer normalizationã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;å±‚å½’ä¸€åŒ–ä¸æ‰¹é‡å½’ä¸€åŒ–&lt;/p&gt;
&lt;p&gt;åœ¨ä¹‹å‰çš„æ–‡ç« æœ‰ä»‹ç´¹é Batch Normalization (BN)ï¼Œå…¶ä½œæ³•æ˜¯åœ¨æ¯ä¸€å€‹ mini-batch çš„ input feature åš normalizeï¼Œé€™æ¨£çš„æ–¹å¼é›–ç„¶åœ¨ CNN ä¸Šç²å¾—äº†å¾ˆå¥½çš„æ•ˆæœï¼Œä½†ä»ç„¶å­˜åœ¨ä¸€äº›ç¼ºé»ï¼šéæ–¼ä¾è³´ batch sizeï¼Œå¦‚æœ batch size å¤ªå°ï¼ŒBN çš„æ•ˆæœæœƒæ˜é¡¯ä¸‹é™ã€‚ä¸å¤ªé©ç”¨æ–¼æ™‚é–“åºåˆ—ï¼Œå› ç‚ºæ–‡æœ¬åºåˆ—çš„é•·åº¦é€šå¸¸ä¸ä¸€è‡´ï¼Œå¼·åˆ¶å°æ¯å€‹æ–‡æœ¬åŸ·è¡Œ BN ä¸å¤§åˆç†ã€‚
å› æ­¤åœ¨ RNN ä¸­è¼ƒå¸¸ä½¿ç”¨ Layer Normalization (LN)ï¼Œæ¦‚å¿µèˆ‡ BN é¡ä¼¼ï¼Œå·®åˆ¥åœ¨æ–¼ LN æ˜¯å°æ¯ä¸€å€‹æ¨£æœ¬é€²è¡Œ normalizeã€‚ç”±ä¸‹åœ–å¯ä»¥å¾ˆæ¸…æ¥šçš„çœ‹å‡ºå…©è€…çš„å·®ç•°ï¼Œå…¶ä¸­æ¯ä¸€è¡Œæ˜¯æŒ‡æ¨£æœ¬ï¼Œæ¯ä¸€åˆ—æ˜¯æ¨£æœ¬ç‰¹å¾µã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ä½ç½®å‰é¦ˆç½‘ç»œï¼ˆå‰é¦ˆï¼‰&lt;/p&gt;
&lt;p&gt;æ¥è‘—é€²å…¥åˆ° FFN å±¤ï¼Œç”±ä¸‹åˆ—å…¬å¼å¯ä»¥çœ‹åˆ°è¼¸å…¥ x å…ˆåšç·šæ€§é‹ç®—å¾Œï¼Œé€å…¥ ReLUï¼Œå†åšä¸€æ¬¡ç·šæ€§é‹ç®—ã€‚å…¶ä¸­è¼¸å…¥è¼¸å‡ºçš„ç¶­åº¦ d_model=512ï¼Œè€Œä¸­é–“å±¤çš„ç¶­åº¦ dff = 2048&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;è§£ç å™¨&#34;&gt;è§£ç å™¨&lt;/h2&gt;
&lt;p&gt;çœ‹åˆ°é€™è£¡å·²ç¶“ç†è§£ Encoder çš„é‹ç®—éç¨‹äº†ï¼Œå†ä¾†çœ‹å³é‚Š Decoder çš„éƒ¨åˆ†å§ï¼&lt;/p&gt;
&lt;p&gt;Decoder èˆ‡ Encoder ä¸€æ¨£æœƒå…ˆè·Ÿ Positional Encoding ç›¸åŠ å†é€²å…¥ layerï¼Œä¸åŒçš„æ˜¯ Decoder æœ‰ä¸‰å€‹å­å±¤ Masked Multi-head Attentionã€Multi-head Attentionã€Feed Forwardã€‚æ­¤å¤–ï¼Œä¸­é–“å±¤ Multi-head Attention çš„è¼¸å…¥ q ä¾†è‡ªæ–¼æœ¬èº«å‰ä¸€å±¤çš„è¼¸å‡ºï¼Œè€Œ k, v å‰‡æ˜¯ä¾†è‡ªæ–¼ Encoder çš„è¼¸å‡ºã€‚&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;æ©ç å¤šå¤´æ³¨æ„ï¼ˆMasked Multi-head Attentionï¼‰&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;ç”±æ–¼å…¶ä»–å…©å±¤è·Ÿ Encoder å¤§è‡´ç›¸åŒï¼Œæ‰€ä»¥å°±è·³éä¾†ä»‹ç´¹ Decoder ä¸­æ‰æœ‰çš„ Masked Multi-head Attentionã€‚&lt;/p&gt;
&lt;p&gt;Transformer çš„ Mask æ©Ÿåˆ¶æœ‰å…©ç¨®ï¼šPadding Maskã€Sequence Mask&lt;/p&gt;
&lt;p&gt;Padding Mask åœ¨ Encoderå’Œ Decoder ä¸­éƒ½æœ‰ä½¿ç”¨åˆ°ï¼Œç›®çš„æ˜¯ç‚ºäº†é™åˆ¶æ¯å€‹è¼¸å…¥çš„é•·åº¦è¦ç›¸åŒï¼Œå°æ–¼è¼ƒçŸ­çš„å¥å­æœƒå°‡ä¸è¶³çš„éƒ¨åˆ†è£œ 0
Sequence Mask åªç”¨æ–¼ Decoderï¼Œç›®çš„æ˜¯ç‚ºäº†é˜²æ­¢æ¨¡å‹çœ‹åˆ°æœªä¾†çš„è³‡è¨Šï¼Œå› æ­¤åœ¨è¶…éç•¶å‰æ™‚åˆ» t çš„è¼¸å‡ºæœƒåŠ ä¸Šä¸€å€‹ maskï¼Œç¢ºä¿æ¨¡å‹çš„é æ¸¬åªä¾è³´å°æ–¼ç•¶å‰æ™‚åˆ»çš„è¼¸å‡ºã€‚
Sequence Mask çš„åšæ³•æ˜¯é€šéä¸€å€‹ä¸Šä¸‰è§’çŸ©é™£ä¾†å¯¦ç¾ï¼Œå°‡é€™äº›å€åŸŸçš„å€¼éƒ½è¨­å®šç‚ºè² ç„¡çª®ï¼Œå¦‚æ­¤ä¸€ä¾†é€™äº›å…ƒç´ ç¶“é softmax å¾Œéƒ½æœƒè®Šç‚º 0 ä»¥é”åˆ° mask çš„æ•ˆæœã€‚&lt;/p&gt;
&lt;h2 id=&#34;é“¾æ¥&#34;&gt;é“¾æ¥&lt;/h2&gt;
&lt;p&gt;ğŸ“ &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1706.03762&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Transformerè®ºæ–‡&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;heading&#34;&gt;&lt;/h2&gt;
</description>
        </item>
        
    </channel>
</rss>
