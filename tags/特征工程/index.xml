<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>特征工程 on 一片生菜叶</title>
        <link>https://sandyness.github.io/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/</link>
        <description>Recent content in 特征工程 on 一片生菜叶</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Wed, 01 Jan 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://sandyness.github.io/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>机器学习笔记(一) - 特征工程</title>
        <link>https://sandyness.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/</link>
        <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
        
        <guid>https://sandyness.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/</guid>
        <description>&lt;p&gt;特征工程，是对原始数据进行一系列的工程处理，去除原始数据的杂质和冗余，将其提炼为可供机器学习的输入特征。机器学习常用的数据类型包括结构化数据和非结构化数据。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;结构化数据&lt;/strong&gt;：可以看作关系型数据库的一张表，每列都有清晰的定义，包含了数值型、类别型两种基本类型;每一行数据表示一个样本的信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;非结构化数据&lt;/strong&gt;：主要包括文本、图像、音频、视频数据，其包含的信息无法用一个简单的数值表示，也没有清晰的类别定义，并且每条数据的大小各不相同。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;数据预处理&#34;&gt;数据预处理&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;处理缺失数据&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;删除缺失值：pandas 中使用 dropna()函数&lt;/li&gt;
&lt;li&gt;补全缺失值
&lt;ul&gt;
&lt;li&gt;pandas 中使用 fillna()函数&lt;/li&gt;
&lt;li&gt;单变量补全
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.impute&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SimpleImputer&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;多变量补全
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.impute&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;IterativeImputer&lt;/span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;KNN补全
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.impute&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;KNNImputer&lt;/span&gt;       
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;删除重复数据&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;特征去重: 对于仅相差常数倍的特征需要进⾏去重处理&lt;/li&gt;
&lt;li&gt;常量特征剔除: 对于常量或⽅差近似为零的特征，其对于样本之间的区分度贡献为零或近似为零&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;异常值检测&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;异常值&lt;/strong&gt;：是指样本中存在的同样本整体差异较⼤的数据，异常数据可以划分为两类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;异常值不属于该总体，而是从另⼀个总体错误抽样到样本中而导致的较⼤差异。&lt;/li&gt;
&lt;li&gt;异常值属于该总体，是由于总体所固有的变异性而导致的较⼤差异。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于数值型的单变量，可以利⽤拉依达准则对其异常值进⾏检测。
假设总体 x 服从正态分布，则： P (|x − μ| &amp;gt; 3σ) ≤ 0.003 其中 μ 表⽰总体的期望，
σ 表⽰总体的标准差。因此，对于样本中出现⼤于 μ + 3σ 或小于 μ − 3σ 的数据的概率是⾮常小的，从而可以对⼤于 μ + 3σ 和小于 μ − 3σ 的数据予以剔除。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;异常检测（Anomaly Detection)&lt;/strong&gt; ：是指对不符合预期模式或数据集中异常项⽬、事件或观测值的识别。
通常异常的样本可能会导致银⾏欺诈、结构缺陷、医疗问题、⽂本错误等不同类型的问题。异常也被称为离群值、噪声、偏差和例外。
常⽤的异常检测算法有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于密度的⽅法：最近邻居法、局部异常因⼦等&lt;/li&gt;
&lt;li&gt;One-Class SVM&lt;/li&gt;
&lt;li&gt;基于聚类的⽅法&lt;/li&gt;
&lt;li&gt;Isolation Forest&lt;/li&gt;
&lt;li&gt;AutoEncoder&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;特征变换和编码&#34;&gt;特征变换和编码&lt;/h2&gt;
&lt;h3 id=&#34;数值类型&#34;&gt;数值类型&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;特征归一化&lt;/strong&gt;: 为了消除数据特征之间的量纲影响，需要对特征进行归一化(Normalization)处理，将所有的特征都统一到一个大致相同的数值区间内，在随机梯度下降的过程中，
特征的更新速度变得更为一致，容易更快地通过梯度下降找到最优解。&lt;/p&gt;
&lt;p&gt;最常用的方法主要有以下两种:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;线性函数归一化(Min-Max Scaling)&lt;/strong&gt;：对原始数据进行线性变换，使结果映射到[0, 1]的范围，实现对原始数据的等比缩放。公式如下：
X= (X - Xmax) / (Xmax - Xmin)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;MinMaxScaler&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;其中X为原始数据，Xmax、Xmin分别为数据最大值和最小值。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;零均值归一化(Z-Score Normalization)&lt;/strong&gt;：它会将原始数据映射到均值为0、标准差为1的分布上。若原始特征的均值为μ、标准差为σ,归一化公式定义为：
z = (X - μ) / σ&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;StandardScaler&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在实际应用中，通过梯度下降法求解的模型通常是需要归一化的，包括&lt;strong&gt;线性回归&lt;/strong&gt;、&lt;strong&gt;逻辑回归&lt;/strong&gt;、&lt;strong&gt;支持向量机&lt;/strong&gt;、&lt;strong&gt;神经网络&lt;/strong&gt;等模型。
但对于&lt;strong&gt;决策树&lt;/strong&gt;模型则并不适用，决策树在进行节点分裂时主要依据数据集和特征的信息增益比，而信息增益比跟特征是否经过归一化是无关的，因为归一化并不会改变样本在特征上的信息增益。&lt;/p&gt;
&lt;h3 id=&#34;类别类型&#34;&gt;类别类型&lt;/h3&gt;
&lt;p&gt;类别型特征(Categorical Feature)是只在有限选项内取值的特征，如性别(男、女)、血型(A、B、 AB、O)等。&lt;/p&gt;
&lt;p&gt;类别型特征原始输入通常是字符串形式，除了决策树等少数模型能直接处理字符串形式的输入，对于逻辑回归、支持向量机等模型来说，类别型特征必须经过处理转换成数值型特征才能正确工作。&lt;/p&gt;
&lt;p&gt;最常用的方法主要有以下两种。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;序号编码（Ordinal Encoding）&lt;/strong&gt;: 常用来处理具有大小关系的类别特征（eg.成绩高中低）&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;OrdinalEncoder&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;独热编码(One-hot Encoding）&lt;/strong&gt;：不具有大小关系的类别特征（eg.血型）&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;OneHotEncoder&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;在独热编码下，特征向量只有某一维取值为1，其他位置取值均为0。因此可以利用向量的稀疏表示有效地节省空间，并且目前大部分的算法均接受稀疏向量形式的输入。&lt;/li&gt;
&lt;li&gt;高维度特征会带来几方面的问题。一是在K近邻算法中，高维空间下两点之间的距离很难得到有效的衡量;二是在逻辑回归模型中，参数的数量会随着维度的增高而增加，容易引起过拟合问题;三是通常只有部分维度是对分类、预测有帮助，因此可以考虑配合特征选择来降低维度。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;二进制编码(Binary Encoding)&lt;/strong&gt;：利用二进制对ID进行哈希映射，最终得到0/1特征向量，且维数少于独热编码，节省了存储空间。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;特征提取和选择&#34;&gt;特征提取和选择&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;特征选择&lt;/strong&gt;是从⼀组特征中选出⼀些最有效的特征，使构造出来的模型更好。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;避免过度拟合，改进预测性能&lt;/li&gt;
&lt;li&gt;使学习器运⾏更快，效能更⾼&lt;/li&gt;
&lt;li&gt;剔除不相关的特征使模型更为简单,容易解释&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;过滤⽅法（Filter Methods）&lt;/strong&gt;：按照发散性或相关性对特征进⾏评分，设定阈值或者待选择阈值的个数，选择特征。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;⽅差选择法：选择⽅差⼤的特征。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;相关关系 &amp;amp; 卡⽅检验：特征与⽬标值的相关关系。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;互信息法：⼀个随机变量包含另⼀个随机变量的信息量。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;封装⽅法（Wrapper Methods）：是利⽤学习算法的性能来评价特征⼦集的优劣。因此，对于⼀个待评价的特征⼦集， Wrapper⽅法需要训练⼀个分类器，根据分类器的性能对该特征⼦集进⾏评价，学习算法包括决策树、神经⽹络、⻉叶斯分类器、近邻法以及⽀持向量机等。Wrapper⽅法缺点主要是特征通⽤性不强，当改变学习算法时，需要针对该学习算法重新进⾏特征选择。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;集成⽅法（Embedded Methods）：在集成法特征选择中，特征选择算法本⾝作为组成部分嵌⼊到学习算法⾥。最典型的是决策树算法。包括基于惩罚项的特征选择法和基于树模型的特征选择法。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# 方差选择法&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.feature_selection&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;VarianceThreshold&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# 根据 k 个最高分选择特征&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.feature_selection&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SelectKBest&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# 卡方检验&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.feature_selection&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chi2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# 基于重要性权重选择特征&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.feature_selection&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SelectFromModel&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;特征监控&#34;&gt;特征监控&lt;/h2&gt;
&lt;p&gt;在数据分析和挖掘中，特征占据着很重要的地位。
因此，我们需要对重要的特征进⾏监控与有效性分析，了解模型所⽤的特征是否存在问题，当某个特别重要的特征出问题时，需要做好备案，防⽌灾难性结果。&lt;/p&gt;
&lt;h2 id=&#34;参考资料&#34;&gt;参考资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://scikit-learn.org/stable/modules/classes.html?highlight=feature_selection#module-sklearn.feature_selection&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;sklearn.feature_selection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;sklearn.preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://scikit-learn.org/stable/modules/classes.html?highlight=sklearn&amp;#43;impute#module-sklearn.impute&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;sklearn.impute&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
