[{"content":"随着深度学习的发展 ，自然语言处理（NLP）技术越来越成熟。在机器翻译、语音识别、情感分析、文本生成、人机对话等任务上都有很好的结果。但由于RNN难以进行并行计算，因此Google提出了一种不使用RNN和CNN，而是使用叫自注意力机制(self-attention mechanism)-Transformer。其网络架构是基于 Seq2Seq + self-attention mechanism。\nSeq2Seq：主要由两篇论文提出：Sequence to Sequence Learning with Neural Networks和 Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation。两者差别是前者使用LSTM，后者使用GRU。\n如下图所示，其架构为 Encoder-Decoder，Encoder 先将输入句子进行编码，得到的状态传给 Decoder 解码生成目标句子。\n注意力机制：但是当讯息太长时，seq2seq容易丢失讯息，因此引入了注意力机制。其概念为将Encoder所有资讯传给Decoder，让Decoder决定将注意力放在哪些资讯上。 有两篇论文：Neural Machine Translation by Jointly Learning to Align and Translate 和 Effective Approaches to Attention-based Neural Machine Translation。\nTransformer 模型架构 Transformer 由 N=6 的 Encoder-Decoder 堆叠而成。Encoder-Decoder 结构有 Multi-head Attention、Add \u0026amp; Norm、Feed Forward、Masked Multi-head Attention。 Encoder（编码器） Positional Encoding（位置编码） input 先经过 embedding 层转换为向量，然后在进入 layer 前先与 Positional Encoding 相加。Positional Encoding 是词语的位置编码，目的是为了让模型考虑词语之间的顺序。\n论文使用 sin, cos 函数进行位置编码，公式如下：\n其中 pos 为词语在序列中的位置、2i, 2i+1 为词语在 Positional Encoding 维度上的 index、d_model 为 Positional Encoding 的维度 (与 Input Embedding 维度相等)，预设值512。\nQ: 为什么要使用 sin, cos 函数进行位置编码?\nsin, cos 函数可以表示两个向量间的线性关系，能呈现不同词语之间的相对位置，并且不受序列长度的限制，不会有重复的问题。 此外，sin, cos 函数有上下界 (位于 [0, 1] 之间)、具有稳定循环的性质。\nMulti-head Attention（多头注意力机制） self attention(自注意机制)\n自注意力机制三个重要的参数 q, k, v。\nq (query): 是指当前的词向量，用于对每个 key 做匹配程度的打分；\nk (key): 是指序列中的所有词向量；\nv (value): 是指实际的序列內容。\n由下图可以看到 q1 会对每个 k 做内积得到的 q , k 之间匹配的相似程度a1,1、a1,2\u0026hellip;,然后做一些列运算得到输出，这些计算步骤称为 Scaled Dot-Product Attention。\nScaled Dot-Product Attention\n由下列公式得知 q, 会先做 Inner Product，得到的值是匹配的相似程度，除以 sqrt(dk) 后，再做 softmax 计算出 v 的权重， 最后与 v 做加权运算，其中 q, k 维度都为 dk，v 维度为 dv。除以 sqrt(dk)是为了避免当 dk (q, k 的维度) 太大时，q, k 的内积（Inner Product）的值过大，softmax 计算的值落入饱和区而导致梯度不稳定。\n多头注意力\n在 CN N中可以输出多个通道。在transformer中，为了模拟CNN能够输出多个通道的效果，提出了多头注意力机制（Multi-head Attention）。\nInput Embedding 和 Positional Encoding 相加后进入到 layer 里。Encoder 有 Multi-head Attention 和 Feed Forward 这两个子层。\n其运算方式与 self-attention mechanism 相同。区别是先将q,k,v拆分成多个低维度的向量。\n若假设head=2,qi会拆分为qi,1、qi,2。最后再把这些head输出concat起来做一次线性运算。这样的好处是能够让各个head(q,k,v)关注不同的资讯，有些关注局部资讯，有些关注全局资讯。\n由下图可以看到q,k,v会做h次的线性映射到低维度的向量，再进行 Scaled Dot-Product Attention，最后将其concat、linear 得到输出。\nAdd \u0026amp; Norm Q: batch normalization 和 layer normalization 的区别?\nbatch normalization 是将每个batch（批量）的特征向量（列）做 normalization，而layer normalization是将每个样本（行）做 normalization。\nQ: 为什么用 layer normalization 而不用 batch normalization?\n在时序的序列里，每个样本的长度可能是不一样的。若用batch normalization对特征向量做均值和方差的计算，会导致结果不稳定。\n经过 Multi-head Attention 后进入 Add \u0026amp; Norm 层，这一层是指 residual connection 和 layer normalization。前一层的输出 Sublayer 与输入 x 相加 (residual connection)，减缓梯度消失的问题，然后再做 layer normalization。\nFeed-Forward Networks (前馈全连接层) 公式如下：\n输入 x 先做线性运算，送入 ReLU，再做一次线性运算。其中输入输出的为度 d_model=512，中间层的维度 dff = 2048。\nDecoder(解码器) Decoder 与 Encoder 一样先 Positional Encoding 相加再进入 layer，区别是 Decoder 有 Masked Multi-head Attention、Multi-head Attention、Feed Forward 这三个子层。 此外， Multi-head Attention 的输入 q 來自自身前一层的输出，而 k, v 则是来自于 Encoder 的输出。 Masked Multi-head Attention（掩码多头机制） Transformer 的 掩码机制有两种：Padding Mask、Sequence Mask\nPadding Mask 在 Encoder 和 Decoder 中都有使用到，目的是为了限制每个输入的长度要相同，对较短的句子会將不足的部分补 0；\nSequence Mask 只用于 Decoder，目的是为了防止模型看到未来的资讯，因此在超过当前时刻t的输出会加一个mask，确保模型的预测只依赖于当前时刻的输出。\nSequence Mask 的做法是通过一个上三角矩阵，将这些区域的值都设为负无穷，这样这些元素经过softmax后都会变为0达到掩码的效果。\n参考链接 论文链接 李沐老师的Transformer【论文精读】 ","date":"2020-09-07T00:00:00Z","permalink":"https://sandyness.github.io/p/transformer/","title":"Transformer"},{"content":"本文介绍CNN的经典模型ResNet。ResNet在2015年微软亚洲院的何凯明博士提出，并在同年的ImageNet LSVRC竞赛中获得了冠军，同时也在ImageNet detection、 ImageNet localization、COCO detection和COCO segmentation等任务中获得第一名，此外还获得了CVPR2016最佳论文奖。 ResNet模型一共使用了152层，其深度比 GoogLeNet 高了七倍多 (20层)，并且模型在ImageNet测试的错误率为 3.6%，人类在 ImageNet 的错误率为 5.1%，模型的错误率已经达到比人类的错误率还要小的程度。\n解决的问题 对很多计算机视觉的任务来说，模型的深度是十分重要的，但是深度模型又难以训练。因为随着模型层数增加到一定程度时，反而会降低训练的准确率。但并不是过拟合，而是深层网络退化的问题。\n网络退化是指层数越来越大时，梯度在做反向传播的过程中消失，导致梯度无法对参数进行跟新。而ResNet为了解决这一问题， 提出了 Residual Learning 让深层网络更容易训练。\n模型架构 什么是Residual Learning？\nResidual Learning 使用 Shortcut Connection 的结构。该结构有两个分支：一个是将输入的 x 跨层传递，另一个是 F(x)。将这两个分支相加再送入激活函数中。输出值为 H(x) = F(x) + x。 当训练达到饱和的时候，F(x) 容易被优化成 0，这时只剩下 x, 此时 H(x) = x，这种情况是 Identify mapping。 什么是Residual Block？\nResidual 采用模组化的方式，对不同的 ResNet，作者提出了两种 Residual Block。\n左图为基本的 Residual Block，使用连续的两个 3 x 3卷积层，并每两个卷积层进行一次 Shortcut Connection，用于 ResNet-34。\n右图则是针对较深的网络所做的改进，称为 bottleneck。因为模型的层数越多，导致训练成本变高，因此为了降低维度，再送入 3 x 3 卷积层之前，先通过 1 x 1 卷积层降低维度，最后再通过 1 x 1卷积 层恢复原本的维度，用于 ResNet-50、ResNet-101、ResNet-152。\n模型架构\n下图为 ResNet 不同层数的网络架构， 第一层为 7x7 的卷积层，接着是 3x3 Maxpooling，再使用大量的 Residual Block，最后使用平均池化 (Global Average Pooling) 并传入全连接层进行分类。 下图为 ResNet-34 的网络架构，虚线部分就是指在做 Shortcut Connection 时，輸入的 x 权重輸出的 F(x) 通道 (channel) 數目不同，因此需要使用1x1 卷积层调整通道维度，使其可以做相加的运算。 代码实现 ResNet 根据不同的深度有不同Residual Block，所以 code 有定义 basic_block 和 bottleneck_block 两组模块。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 class basic_block(nn.Module): # 输出通道的倍数 expansion = 1 def __init__(self, in_channels, out_channels, stride, downsample): super(basic_block, self).__init__() self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=stride, padding=1, bias=False) self.bn1 = nn.BatchNorm2d(out_channels) self.relu = nn.ReLU(inplace=True) self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1, bias=False) self.bn2 = nn.BatchNorm2d(out_channels) # 在 shortcut 时，若维度不一样，则要修改维度 self.downsample = downsample def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample is not None: residual = self.downsample(x) out += residual out = self.relu(out) return out class bottleneck_block(nn.Module): # 输出通道的倍数 expansion = 4 def __init__(self, in_channels, out_channels, stride, downsample): super(bottleneck_block, self).__init__() self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, bias=False) self.bn1 = nn.BatchNorm2d(out_channels) self.relu = nn.ReLU(inplace=True) self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=stride, padding=1, bias=False) self.bn2 = nn.BatchNorm2d(out_channels) self.conv3 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels * self.expansion, kernel_size=1, bias=False) self.bn3 = nn.BatchNorm2d(out_channels * self.expansion) # 在 shortcut 时，若维度不一样，则要修改维度 self.downsample = downsample def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.relu(out) out = self.conv3(out) out = self.bn3(out) if self.downsample is not None: residual = self.downsample(x) out += residual out = self.relu(out) return out class ResNet(nn.Module): def __init__(self, net_block, layers, num_classes): super(ResNet, self).__init__() self.in_channels = 64 self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False) self.bn1 = nn.BatchNorm2d(64) self.relu = nn.ReLU(inplace=True) self.maxpooling = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) self.layer1 = self.net_block_layer(net_block, 64, layers[0]) self.layer2 = self.net_block_layer(net_block, 128, layers[1], stride=2) self.layer3 = self.net_block_layer(net_block, 256, layers[2], stride=2) self.layer4 = self.net_block_layer(net_block, 512, layers[3], stride=2) self.avgpooling = nn.AvgPool2d(7, stride=1) self.fc = nn.Linear(512 * net_block.expansion, num_classes) # 参数初始化 for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode=\u0026#34;fan_out\u0026#34;, nonlinearity=\u0026#34;relu\u0026#34;) elif isinstance(m, nn.BatchNorm2d): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) def net_block_layer(self, net_block, out_channels, num_blocks, stride=1): downsample = None # 在 shortcut 时，若维度不一样，则要修改维度 if stride != 1 or self.in_channels != out_channels * net_block.expansion: downsample = nn.Sequential(nn.Conv2d(self.in_channels, out_channels * net_block.expansion, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(out_channels * net_block.expansion)) layers = [] layers.append(net_block(self.in_channels, out_channels, stride, downsample)) if net_block.expansion != 1: self.in_channels = out_channels * net_block.expansion else: self.in_channels = out_channels for i in range(1, num_blocks): layers.append(net_block(self.in_channels, out_channels, 1, None)) return nn.Sequential(*layers) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.maxpooling(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) x = self.avgpooling(x) x = torch.flatten(x, start_dim=1) x = self.fc(x) return x def ResNet_n(num_layers): if num_layers == 18: # ResNet18 model = ResNet(basic_block, [2, 2, 2, 2], num_classes) elif num_layers == 34: # ResNet34 model = ResNet(basic_block, [3, 4, 6, 3], num_classes) elif num_layers == 50: # ResNet50 model = ResNet(bottleneck_block, [3, 4, 6, 3], num_classes) elif num_layers == 101: # ResNet101 model = ResNet(bottleneck_block, [3, 4, 23, 3], num_classes) elif num_layers == 152: # ResNet152 model = ResNet(bottleneck_block, [3, 8, 36, 3], num_classes) else: print(\u0026#34;error\u0026#34;) return return model 参考链接 论文链接 ResNet【论文精读】 ","date":"2020-07-08T00:00:00Z","permalink":"https://sandyness.github.io/p/cnnresnet/","title":"CNN｜ResNet"},{"content":"AlexNet 是 Alex Krizhevsky 于2012提出的。并在同年的 ImageNet LSVRC 竞赛中获得了冠军。AlexNet 架构有8层，其中5个卷积层、3个全连接层。\n模型架构 第1层到第5层是卷积层。其中第1、2、5卷积层使用size为3 x 3、stride为2的Maxpooling，因为 stride(2) \u0026lt; size(3), pooling 可以重叠，所以能够重复检视特征，避免重要的特征被舍弃掉。还能够避免过拟合。第6层到第8层使用全连接层。\nInput size\n将输入尺寸变大，可以输入 224 x 224 尺寸的彩色图片。\nKernel size\n由于输入层变大，所以将第一层的 kernel 设定为 11 x 11, stride为4，第二层则使用 size为5 x 5的 kernel, 之后的层都使用size为3 x 3 ，stride 为1的 kernel。\nActive function\n使用 ReLU 作为激活函数，避免因为神经网络层数过深或梯度过小，导致梯度消失(Vanishing gradient) 的问题。相比 Sigmoid/Tanh，ReLU收敛速度更快。\n避免过拟合的方法\nDropout\n是指在每一次训练过程中，随机选取神经元使其不参与训练。可以让模型不过度依赖某些特征，增强模型的泛化能力。 AlexNet在第6、7层的全连接层中加入了Dropout,设定值为0.5。\n图像增强\n使用随机裁剪、水平变换、颜色变化的方法进行了数据增强。\n将大小为256 x 256的图片进行随机裁剪大小为224 x 224，再进行水平翻转； 对RGB通道做主成分分析（PCA），再使用高斯分布，对颜色和亮度进行变换。 GPU\n因为单个GTX580 GPU无法负荷大数据量的运算，AlexNet使用了GTX580 GPU 进行同步训练，加快了训练速度。\n代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class AlexNet(nn.Module): def __init__(self, num_classes): super(AlexNet, self).__init__() self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=11, padding=2, stride=4) self.conv2 = nn.Conv2d(in_channels=64, out_channels=192, kernel_size=5, padding=2) self.conv3 = nn.Conv2d(in_channels=192, out_channels=384, kernel_size=3, padding=1) self.conv4 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1) self.conv5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1) self.fc1 = nn.Linear(in_features=256*6*6, out_features=4096) self.fc2 = nn.Linear(in_features=4096, out_features=1024) self.fc3 = nn.Linear(in_features=1024, out_features=num_classes) def forward(self, x): x = F.relu(self.conv1(x)) x = F.max_pool2d(x, kernel_size=3, stride=2) x = F.relu(self.conv2(x)) x = F.max_pool2d(x, kernel_size=3, stride=2) x = F.relu(self.conv3(x)) x = F.relu(self.conv4(x)) x = F.relu(self.conv5(x)) x = F.max_pool2d(x, kernel_size=3, stride=2) # x = x.view(x.size(0), -1) x = torch.flatten(x, start_dim=1) x = F.relu(self.fc1(x)) x = F.dropout(x, p=0.5) x = F.relu(self.fc2(x)) x = F.dropout(x, p=0.5) x = self.fc3(x) return x 参考链接 论文链接 AlexNet【论文精读】 ","date":"2020-05-05T00:00:00Z","permalink":"https://sandyness.github.io/p/cnnalexnet/","title":"CNN｜AlexNet"},{"content":"什么是学习？为什么学习？ 学习是由好奇心驱动的，学习的目的是帮助我们解决实际的问题。\n认识大脑是如何学习的 大脑的结构和运行都会应对各种不同的心理训练而改变。 由训练引起的认知和生理变化需要继续保持，如果停止训练，它们便逐渐消失。 专注模式和发散模式 专注模式：专注的练习和重复是创造记忆痕迹。在专注模式下工作，就像是为砌墙提供砖块。 发散模式：发散模式则是用泥浆把砖块逐渐结合在一起。让学习更有深度和创造力。不断增加挫败感是个有益的暂停信号，暗示应该转到发散模式了。 工作记忆和长期记忆 工作记忆：在大脑中对正在处理的信息进行瞬时以及有意识加工的这部分记忆。只能一次扔四个球的杂耍演员。 长期记忆：可以看作仓库。东西一旦存进去，它们通常就一直待在那了。在学习数学和科学方面也很重要，解题时需要的基本概念和技巧都存储在那里。能存储大量知识的仓库，要靠定期回访保持对其中内容的新鲜感。 学习如何学习 想要达成你的学习目标，不要想着如何更努力，而是找到做正确的方法，更聪明的学习。掌握「更有效率和效果」的学习计划、学习方法、和学习策略。天赋和智力不是必要条件。\n很多事情只是你现在「还不擅长」而已。重要的是「成长心态」，无论你对那一个学科的天赋如何，只要懂得投入时间和精力，加上适当指导，便能精通任何领域。特定的方式不是必要条件。每个人都有适合自己的学习方式。特定的动机不是必要条件。如果你想等到灵光乍现的动机出现，才开始学习的话，你可能会在等待中错过很多事。重点在于「自信」，你相信自己有能力达成目标、克服困难，相信自己能够学会任何一项你真正在乎的事情。\n学习的「总时间」是次要因素，学习的「高品质」才是主要因素。宏观计划与你把时间花在学习的「原因」有关，是为了确保目标是你真正想要的。在这个阶段非常重要，要知道自己的总体学习目标和目的，确保即将花费的学习时间，可以创造出你想要的结果。\n为什么学？ 工具性动机 内在性动机 学什么？ 概念 事实 程序 制定目标和计划 你跟目标之间的知识落差 达成目标的方式 画出你的学习蓝图 制定可衡量的目标 反思和回顾学习成效 目标具体明确，计划的可执行性，可量化且难度适中 学习tips 专注 重复:间隔复习所学的内容，对问题形成组块，让每一重复都有所提升 回想：在学过一个概念和科学知识后，先在脑袋中进行回想练习，然后再学习再进行回想练习。 自我测试 输出：向他人讲解（费曼学习法），输出是最好的应用 及时反馈并自我纠正，创建有反馈的训练工具 找位好导师 随时监测并追踪你的提高方法并保持动机 走出舒适区：做之前没做过的事情，而不是更难的事情。 睡眠：睡眠时记忆和学习的重要环节。清除琐碎的记忆，增强重要部分。提升解决难题，理解知识的能力。让精力充沛的大脑阅读一个小时，强过疲惫的大脑读上三个小时。 不要拖延：困难的事情最先做 环境 在能自主创造的环境里工作效率最高，用大屏幕有助于记忆 练习在不同场所学习，不同的环境可以帮助换个角度看问题。 案例：如何学习数据科学 设定目标 精通数据python 精通统计学 精通机器学习的各种模型和算法 制定计划 周计划(60h~70h）：two topic/week 日计划（10h) study two things a day(技术 \u0026amp; 非技术) main topic you focus，one topic review give each topic at least a week repeat all the topic at least once 看了Tina最新一期关于如何快速学习编程的三个方法，觉得很有帮助，分享给大家： 直接学习：通过直接时间来学习你想学的东西，学习code应该是在了解一些基础知识后直接基于项目来coding 在学习过程中不要记太多的notes 在学习编程的初期，先学高级编程语言（比如python、javascript、java） 将复杂的问题横向纵向（分层）拆解，不要害怕麻烦（大工程→小工程） Reference 书\n《学习之道》 《刻意练习》 《超速学习者》 《在大脑外学习》 文章\n像运动员一样学习\n自学是门手艺\n85% for learning\n","date":"2020-04-02T00:00:00Z","permalink":"https://sandyness.github.io/p/%E6%95%88%E7%8E%87%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0/","title":"效率｜如何学习"},{"content":"模型性能评估 对模型的泛化性能进⾏评估，不仅需要有效可⾏的实验评估⽅法，还需要有衡量模型泛化能⼒的评价标准，也就是性 能度量（performance measure）。性能度量反映了任务需求，在对⽐不同模型的能⼒时，使⽤不同的性能度量往往会导 致不同的评判结果，这意味着模型的“好坏”是相对的，什么样的模型时好的，不仅仅取决于算法和数据，还决定于任务需求。\n回归问题\n平均绝对误差（mean absolute error, MAE） 平均绝对百分⽐误差（mean absolute percentage error, MAPE） 均⽅误差（mean squared error, MSE） 均⽅根误差（root-mean-square error, RMSE） 误差标准差 分类问题\n误差和精度 准确率，召回率和 F1 Score ROC 和 AUC 多分类 Log Loss 聚类问题\n外部指标\nJaccard 系数（Jaccard Coefficient，JC） FM 指数（Fowlkes and Mallows Index，FMI） Rand 指数（Rand Index，RI） 内部指标：考虑聚类结果的簇划分 ，定义：C = {C1,C2,\u0026hellip;,Ck}\n簇C内样本间的平均距离 簇C内样本间最远距离 簇Ci与Cj最近样本件的距离 簇Ci与Cj中⼼点间的距离 DB 指数（Davies-Bouldin Index，DBI）类⽐簇间相似度 Dunn 指数（Dunn Index，DI）类⽐簇内相似度 DBI 的值越小越好，DI 相反，越⼤越好。 模型生成和选择 过拟合问题\nL1正则化 L2正则化 评估方法\n留出法 交叉验证 ⾃助法 偏差和方差\n泛化误差可以分解为偏差、⽅差与噪声之和。\n偏差：度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本⾝的拟合能⼒。\n⽅差：度量了同样⼤小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响。\n噪声：则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本⾝的难度。\n偏差-⽅差分解说明，泛化性能是由学习算法的能⼒、数据的充分性以及学习任务本⾝的难度所共同决定的。给定学习任务，为了取得好的泛化性能，需要使偏差较小，即能够充分拟合数据，且使⽅差较小，即使数据扰动产⽣的影响较小。\n超参数优化 模型的参数和超参数⼆者有着本质上的区别：模型参数是模型内部的配置变量，可以⽤数据估计模型参数的值，例如： 回归中的权重，决策树分类点的阈值等；模型超参数是模型外部的配置，必须⼿动设置参数的值，例如：随机森林树的 个数，聚类⽅法⾥⾯类的个数，或者主题模型⾥⾯主题的个数等。\n搜索算法: ⽹格搜索，随机搜索等\nGrid Search 是⼀个暴⼒解法，通过所有需要测试的超参数，找出所有可能的超参数组合，最根据验证集的损失找出最好的⼀组超参数。这是⼀个⾮常消耗资源的⽅ 法。如果有个超参数，每个超参数选最多个可能值，那么该算法的时间复杂度是 。 Random Search 的使⽤⽅法 Grid Search 基本⼀致，区别在于 Random Search 会在超参数的组合空间内随机采样搜索，其搜索能⼒取决于设定的抽样次数，最重要的是收敛更快。 启发式算法: 遗传算法，粒⼦群算法等\n启发式算法（Heuristic Algorithms）是相对于最优算法提出的。 ⼀个问题的最优算法是指求得该问题每个实例的最优解。 启发式算法可以这样定义：⼀个基于直观或经验构造的算法，在可接受的花费（指计算时间、占⽤空间等）下给出待解决组合优化问题每⼀个实例的⼀个可⾏解，该可⾏解与最优解的偏离程度不⼀定事先可以预计。 在某些情况下，特别是实际问题中，最优算法的计算时间使⼈⽆法忍受或因问题的难度使其计算时间随问题规模的增加以指数速度增加，此时只能通过启发式算法求得问题的⼀个可⾏解。\n贝叶斯优化: ⾼斯过程，TPE 等\nGrid Search 和 Randomized Search 可以让整个调参过程⾃动化，但它们⽆法从之前的调参结果中获取信息，可能会尝试很多⽆效的参数空间。 而⻉叶斯优化，会对上⼀次的评估结果进⾏追踪，建⽴⼀个概率模型，反应超参数在⽬标函数上表现的概率分布⽤于指导下⼀次的参数选择。 ⻉叶斯优化适⽤于随机、⾮凸、不连续⽅程的优化。Sequential Model-Based Optimization (SMBO) 是⻉叶斯优化更具体的表现形式，⼀般会有以下⼏个过程：\n给定要搜索的超参数空间 定义⼀个⽬标函数⽤于评估优化 建⽴⽬标函数的 Surrogate Model 建⽴⼀个选择超参数的标准的评估 Surrogate Model 获取评分和超参数的样本⽤于更新 Surrogate Model ","date":"2020-01-02T00:00:00Z","permalink":"https://sandyness.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%BA%8C-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E5%92%8C%E8%B6%85%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96/","title":"机器学习笔记（二）- 模型评估和超参数优化"},{"content":"特征工程，是对原始数据进行一系列的工程处理，去除原始数据的杂质和冗余，将其提炼为可供机器学习的输入特征。机器学习常用的数据类型包括结构化数据和非结构化数据。\n结构化数据：可以看作关系型数据库的一张表，每列都有清晰的定义，包含了数值型、类别型两种基本类型;每一行数据表示一个样本的信息。 非结构化数据：主要包括文本、图像、音频、视频数据，其包含的信息无法用一个简单的数值表示，也没有清晰的类别定义，并且每条数据的大小各不相同。 数据预处理 处理缺失数据\n删除缺失值：pandas 中使用 dropna()函数 补全缺失值 pandas 中使用 fillna()函数 单变量补全 1 from sklearn.impute import SimpleImputer 多变量补全 1 from sklearn.impute import IterativeImputer KNN补全 1 from sklearn.impute import KNNImputer 删除重复数据\n特征去重: 对于仅相差常数倍的特征需要进⾏去重处理 常量特征剔除: 对于常量或⽅差近似为零的特征，其对于样本之间的区分度贡献为零或近似为零 异常值检测\n异常值：是指样本中存在的同样本整体差异较⼤的数据，异常数据可以划分为两类：\n异常值不属于该总体，而是从另⼀个总体错误抽样到样本中而导致的较⼤差异。 异常值属于该总体，是由于总体所固有的变异性而导致的较⼤差异。 对于数值型的单变量，可以利⽤拉依达准则对其异常值进⾏检测。 假设总体 x 服从正态分布，则： P (|x − μ| \u0026gt; 3σ) ≤ 0.003 其中 μ 表⽰总体的期望， σ 表⽰总体的标准差。因此，对于样本中出现⼤于 μ + 3σ 或小于 μ − 3σ 的数据的概率是⾮常小的，从而可以对⼤于 μ + 3σ 和小于 μ − 3σ 的数据予以剔除。\n异常检测（Anomaly Detection) ：是指对不符合预期模式或数据集中异常项⽬、事件或观测值的识别。 通常异常的样本可能会导致银⾏欺诈、结构缺陷、医疗问题、⽂本错误等不同类型的问题。异常也被称为离群值、噪声、偏差和例外。 常⽤的异常检测算法有：\n基于密度的⽅法：最近邻居法、局部异常因⼦等 One-Class SVM 基于聚类的⽅法 Isolation Forest AutoEncoder 特征变换和编码 数值类型 特征归一化: 为了消除数据特征之间的量纲影响，需要对特征进行归一化(Normalization)处理，将所有的特征都统一到一个大致相同的数值区间内，在随机梯度下降的过程中， 特征的更新速度变得更为一致，容易更快地通过梯度下降找到最优解。\n最常用的方法主要有以下两种:\n线性函数归一化(Min-Max Scaling)：对原始数据进行线性变换，使结果映射到[0, 1]的范围，实现对原始数据的等比缩放。公式如下： X= (X - Xmax) / (Xmax - Xmin)\n1 from sklearn.preprocessing import MinMaxScaler 其中X为原始数据，Xmax、Xmin分别为数据最大值和最小值。\n零均值归一化(Z-Score Normalization)：它会将原始数据映射到均值为0、标准差为1的分布上。若原始特征的均值为μ、标准差为σ,归一化公式定义为： z = (X - μ) / σ\n1 from sklearn.preprocessing import StandardScaler 在实际应用中，通过梯度下降法求解的模型通常是需要归一化的，包括线性回归、逻辑回归、支持向量机、神经网络等模型。 但对于决策树模型则并不适用，决策树在进行节点分裂时主要依据数据集和特征的信息增益比，而信息增益比跟特征是否经过归一化是无关的，因为归一化并不会改变样本在特征上的信息增益。\n类别类型 类别型特征(Categorical Feature)是只在有限选项内取值的特征，如性别(男、女)、血型(A、B、 AB、O)等。\n类别型特征原始输入通常是字符串形式，除了决策树等少数模型能直接处理字符串形式的输入，对于逻辑回归、支持向量机等模型来说，类别型特征必须经过处理转换成数值型特征才能正确工作。\n最常用的方法主要有以下两种。\n序号编码（Ordinal Encoding）: 常用来处理具有大小关系的类别特征（eg.成绩高中低）\n1 from sklearn.preprocessing import OrdinalEncoder 独热编码(One-hot Encoding）：不具有大小关系的类别特征（eg.血型）\n1 from sklearn.preprocessing import OneHotEncoder 在独热编码下，特征向量只有某一维取值为1，其他位置取值均为0。因此可以利用向量的稀疏表示有效地节省空间，并且目前大部分的算法均接受稀疏向量形式的输入。 高维度特征会带来几方面的问题。一是在K近邻算法中，高维空间下两点之间的距离很难得到有效的衡量;二是在逻辑回归模型中，参数的数量会随着维度的增高而增加，容易引起过拟合问题;三是通常只有部分维度是对分类、预测有帮助，因此可以考虑配合特征选择来降低维度。 二进制编码(Binary Encoding)：利用二进制对ID进行哈希映射，最终得到0/1特征向量，且维数少于独热编码，节省了存储空间。\n特征提取和选择 特征选择是从⼀组特征中选出⼀些最有效的特征，使构造出来的模型更好。\n避免过度拟合，改进预测性能 使学习器运⾏更快，效能更⾼ 剔除不相关的特征使模型更为简单,容易解释 过滤⽅法（Filter Methods）：按照发散性或相关性对特征进⾏评分，设定阈值或者待选择阈值的个数，选择特征。\n⽅差选择法：选择⽅差⼤的特征。\n相关关系 \u0026amp; 卡⽅检验：特征与⽬标值的相关关系。\n互信息法：⼀个随机变量包含另⼀个随机变量的信息量。\n封装⽅法（Wrapper Methods）：是利⽤学习算法的性能来评价特征⼦集的优劣。因此，对于⼀个待评价的特征⼦集， Wrapper⽅法需要训练⼀个分类器，根据分类器的性能对该特征⼦集进⾏评价，学习算法包括决策树、神经⽹络、⻉叶斯分类器、近邻法以及⽀持向量机等。Wrapper⽅法缺点主要是特征通⽤性不强，当改变学习算法时，需要针对该学习算法重新进⾏特征选择。\n集成⽅法（Embedded Methods）：在集成法特征选择中，特征选择算法本⾝作为组成部分嵌⼊到学习算法⾥。最典型的是决策树算法。包括基于惩罚项的特征选择法和基于树模型的特征选择法。\n1 2 3 4 5 6 7 8 9 10 11 # 方差选择法 from sklearn.feature_selection import VarianceThreshold # 根据 k 个最高分选择特征 from sklearn.feature_selection import SelectKBest # 卡方检验 from sklearn.feature_selection import chi2 # 基于重要性权重选择特征 from sklearn.feature_selection import SelectFromModel 特征监控 在数据分析和挖掘中，特征占据着很重要的地位。 因此，我们需要对重要的特征进⾏监控与有效性分析，了解模型所⽤的特征是否存在问题，当某个特别重要的特征出问题时，需要做好备案，防⽌灾难性结果。\n参考资料 sklearn.feature_selection sklearn.preprocessing sklearn.impute ","date":"2020-01-01T00:00:00Z","permalink":"https://sandyness.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/","title":"机器学习笔记(一) - 特征工程"}]