[{"content":"一、什么是学习？为什么学习？ 学习是由好奇心驱动的，学习的目的是帮助我们解决实际的问题。\n想要达成你的学习目标，不要想着如何更努力，而是找到做正确的方法，更聪明的学习。掌握「更有效率和效果」的学习计划、学习方法、和学习策略。天赋和智力不是必要条件。\n很多事情只是你现在「还不擅长」而已。重要的是「成长心态」，无论你对那一个学科的天赋如何，只要懂得投入时间和精力，加上适当指导，便能精通任何领域。特定的方式不是必要条件。每个人都有适合自己的学习方式。特定的动机不是必要条件。如果你想等到灵光乍现的动机出现，才开始学习的话，你可能会在等待中错过很多事。重点在于「自信」，你相信自己有能力达成目标、克服困难，相信自己能够学会任何一项你真正在乎的事情。\n学习的「总时间」是次要因素，学习的「高品质」才是主要因素。宏观计划与你把时间花在学习的「原因」有关，是为了确保目标是你真正想要的。在这个阶段非常重要，你要知道自己的总体学习目标和目的，确保即将花费的学习时间，可以创造出你想要的结果。\n二、认识大脑是如何学习的 大脑的结构和运行都会应对各种不同的心理训练而改变。 由训练引起的认知和生理变化需要继续保持，如果停止训练，它们便逐渐消失。 对于刻意练习，不仅要发掘自己的潜能，还要构筑它。 专注模式和发散模式 专注的练习和重复是创造记忆痕迹。在专注模式下工作，就像是为砌墙提供砖块，发散模式则是用泥浆把砖块逐渐结合在一起。 发散模式让学习更有深度和创造力。不断增加挫败感是个有益的暂停信号，暗示应该转到发散模式了。 工作记忆和长期记忆 工作记忆：在大脑中对正在处理的信息进行瞬时以及有意识加工的这部分记忆。只能一次扔四个球的杂耍演员。 长期记忆：可以看作仓库。东西一旦存进去，它们通常就一直待在那了。在学习数学和科学方面也很重要，解题时需要的基本概念和技巧都存储在那里。能存储大量知识的仓库，要靠定期回访保持对其中内容的新鲜感。 三、学习如何学习 为什么学？\n工具性动机 内在性动机 你到底要学什么？\n概念 事实 程序 制定目标和计划\n你跟目标之间的知识落差 达成目标的方式 画出你的学习蓝图 制定可衡量的目标 反思和回顾学习成效 目标具体明确，计划的可执行性，可量化且难度适中 专注\n**重复：**间隔复习所学的内容，对问题形成组块，让每一重复都有所提升\n回想：在学过一个概念和科学知识后，先在脑袋中进行回想练习，然后再学习再进行回想练习。 自我测试 输出：向他人讲解（费曼学习法），输出是最好的应用 及时反馈并自我纠正，创建有反馈的训练工具\n找位好导师 随时监测并追踪你的提高方法并保持动机 穿插学习法\n在练习中交替使用不同的解题技巧\n走出舒适区\n做之前没做过的事情，而不是更难的事情。\n睡眠\n睡眠时记忆和学习的重要环节。清除琐碎的记忆，增强重要部分。提升解决难题，理解知识的能力。让精力充沛的大脑阅读一个小时，强过疲惫的大脑读上三个小时。\n不要拖延\n困难的事情最先做\n大脑外学习\n关系：通过教别人让自己更好的思考 环境： 在能自主创造的环境里工作效率最高，用大屏幕有助于记忆 设计你的环境 练习在不同场所学习，不同的环境可以帮助换个角度看问题。 身体：手势的重要性 六、案例：我是如何学习数据科学的 设定目标 精通数据python 精通统计学 精通机器学习的各种模型和算法 制定计划 周计划(60h~70h) two topic/week 日计划（10h) study two things a day(技术 \u0026amp; 非技术) main topic you focus，one topic review give each topic at least a week repeat all the topic at least once 停止衡量你对任何学习的感受，当下的失败只是不习惯算法思维，看不到对现实生活的任何实用性。 看了Tina最新一期关于如何快速学习编程的三个方法，觉得很有帮助，分享给大家： 直接学习：通过直接时间来学习你想学的东西，学习code应该是在了解一些基础知识后直接基于项目来coding 在学习过程中不要记太多的notes 在学习编程的初期，先学高级编程语言（比如python、javascript、java） 将复杂的问题横向纵向（分层）拆解，不要害怕麻烦（大工程→小工程） 学-练-用-造 学：读不懂也要读完，然后重复很多遍 绝不做预算不够的事情 六、参考书籍和文章 《学习之道》\n《刻意练习》\n《超速学习者》\n《在大脑外学习》\n《像运动员一样学习》\n《自学是门手艺》\n85% for learnning\n","date":"2022-09-12T00:00:00Z","permalink":"https://sandyness.github.io/p/%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0/","title":"如何学习"},{"content":"Model Generator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class Generator(nn.Module): \u0026#34;\u0026#34;\u0026#34; Input shape: (batch, in_dim) Output shape: (batch, 3, 64, 64) \u0026#34;\u0026#34;\u0026#34; def __init__(self, in_dim, feature_dim=64): super().__init__() #input: (batch, 100) self.l1 = nn.Sequential( nn.Linear(in_dim, feature_dim * 8 * 4 * 4, bias=False), nn.BatchNorm1d(feature_dim * 8 * 4 * 4), nn.ReLU() ) self.l2 = nn.Sequential( self.dconv_bn_relu(feature_dim * 8, feature_dim * 4), #(batch, feature_dim * 16, 8, 8) self.dconv_bn_relu(feature_dim * 4, feature_dim * 2), #(batch, feature_dim * 16, 16, 16) self.dconv_bn_relu(feature_dim * 2, feature_dim), #(batch, feature_dim * 16, 32, 32) ) self.l3 = nn.Sequential( nn.ConvTranspose2d(feature_dim, 3, kernel_size=5, stride=2, padding=2, output_padding=1, bias=False), nn.Tanh() ) self.apply(weights_init) def dconv_bn_relu(self, in_dim, out_dim): return nn.Sequential( nn.ConvTranspose2d(in_dim, out_dim, kernel_size=5, stride=2, padding=2, output_padding=1, bias=False), #double height and width nn.BatchNorm2d(out_dim), nn.ReLU(True) ) def forward(self, x): y = self.l1(x) y = y.view(y.size(0), -1, 4, 4) y = self.l2(y) y = self.l3(y) return y Discriminator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class Discriminator(nn.Module): \u0026#34;\u0026#34;\u0026#34; Input shape: (batch, 3, 64, 64) Output shape: (batch) \u0026#34;\u0026#34;\u0026#34; def __init__(self, in_dim, feature_dim=64): super(Discriminator, self).__init__() #input: (batch, 3, 64, 64) \u0026#34;\u0026#34;\u0026#34; NOTE FOR SETTING DISCRIMINATOR: Remove last sigmoid layer for WGAN \u0026#34;\u0026#34;\u0026#34; self.l1 = nn.Sequential( nn.Conv2d(in_dim, feature_dim, kernel_size=4, stride=2, padding=1), #(batch, 3, 32, 32) nn.LeakyReLU(0.2), self.conv_bn_lrelu(feature_dim, feature_dim * 2), #(batch, 3, 16, 16) self.conv_bn_lrelu(feature_dim * 2, feature_dim * 4), #(batch, 3, 8, 8) self.conv_bn_lrelu(feature_dim * 4, feature_dim * 8), #(batch, 3, 4, 4) nn.Conv2d(feature_dim * 8, 1, kernel_size=4, stride=1, padding=0), nn.Sigmoid() ) self.apply(weights_init) def conv_bn_lrelu(self, in_dim, out_dim): \u0026#34;\u0026#34;\u0026#34; NOTE FOR SETTING DISCRIMINATOR: You can\u0026#39;t use nn.Batchnorm for WGAN-GP Use nn.InstanceNorm2d instead \u0026#34;\u0026#34;\u0026#34; return nn.Sequential( nn.Conv2d(in_dim, out_dim, 4, 2, 1), nn.BatchNorm2d(out_dim), nn.LeakyReLU(0.2), ) def forward(self, x): y = self.l1(x) y = y.view(-1) return y ","date":"2021-08-08T00:00:00Z","permalink":"https://sandyness.github.io/p/generative-adversarial-network/","title":"Generative Adversarial Network"}]