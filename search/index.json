[{"content":"什么是学习？为什么学习？ 学习是由好奇心驱动的，学习的目的是帮助我们解决实际的问题。\n想要达成你的学习目标，不要想着如何更努力，而是找到做正确的方法，更聪明的学习。掌握「更有效率和效果」的学习计划、学习方法、和学习策略。天赋和智力不是必要条件。\n很多事情只是你现在「还不擅长」而已。重要的是「成长心态」，无论你对那一个学科的天赋如何，只要懂得投入时间和精力，加上适当指导，便能精通任何领域。特定的方式不是必要条件。每个人都有适合自己的学习方式。特定的动机不是必要条件。如果你想等到灵光乍现的动机出现，才开始学习的话，你可能会在等待中错过很多事。重点在于「自信」，你相信自己有能力达成目标、克服困难，相信自己能够学会任何一项你真正在乎的事情。\n学习的「总时间」是次要因素，学习的「高品质」才是主要因素。宏观计划与你把时间花在学习的「原因」有关，是为了确保目标是你真正想要的。在这个阶段非常重要，你要知道自己的总体学习目标和目的，确保即将花费的学习时间，可以创造出你想要的结果。\n认识大脑是如何学习的 大脑的结构和运行都会应对各种不同的心理训练而改变。 由训练引起的认知和生理变化需要继续保持，如果停止训练，它们便逐渐消失。 对于刻意练习，不仅要发掘自己的潜能，还要构筑它。 专注模式和发散模式 专注的练习和重复是创造记忆痕迹。在专注模式下工作，就像是为砌墙提供砖块，发散模式则是用泥浆把砖块逐渐结合在一起。 发散模式让学习更有深度和创造力。不断增加挫败感是个有益的暂停信号，暗示应该转到发散模式了。 工作记忆和长期记忆 工作记忆：在大脑中对正在处理的信息进行瞬时以及有意识加工的这部分记忆。只能一次扔四个球的杂耍演员。 长期记忆：可以看作仓库。东西一旦存进去，它们通常就一直待在那了。在学习数学和科学方面也很重要，解题时需要的基本概念和技巧都存储在那里。能存储大量知识的仓库，要靠定期回访保持对其中内容的新鲜感。 学习如何学习 为什么学？\n工具性动机 内在性动机 你到底要学什么？\n概念 事实 程序 制定目标和计划\n你跟目标之间的知识落差 达成目标的方式 画出你的学习蓝图 制定可衡量的目标 反思和回顾学习成效 目标具体明确，计划的可执行性，可量化且难度适中 专注\n重复:间隔复习所学的内容，对问题形成组块，让每一重复都有所提升\n回想：在学过一个概念和科学知识后，先在脑袋中进行回想练习，然后再学习再进行回想练习。 自我测试 输出：向他人讲解（费曼学习法），输出是最好的应用 及时反馈并自我纠正，创建有反馈的训练工具\n找位好导师 随时监测并追踪你的提高方法并保持动机 穿插学习法\n在练习中交替使用不同的解题技巧\n走出舒适区\n做之前没做过的事情，而不是更难的事情。\n睡眠\n睡眠时记忆和学习的重要环节。清除琐碎的记忆，增强重要部分。提升解决难题，理解知识的能力。让精力充沛的大脑阅读一个小时，强过疲惫的大脑读上三个小时。\n不要拖延\n困难的事情最先做\n大脑外学习\n关系：通过教别人让自己更好的思考 环境： 在能自主创造的环境里工作效率最高，用大屏幕有助于记忆 设计你的环境 练习在不同场所学习，不同的环境可以帮助换个角度看问题。 身体：手势的重要性 案例：我是如何学习数据科学的 设定目标 精通数据python 精通统计学 精通机器学习的各种模型和算法 制定计划 周计划(60h~70h) two topic/week 日计划（10h) study two things a day(技术 \u0026amp; 非技术) main topic you focus，one topic review give each topic at least a week repeat all the topic at least once 停止衡量你对任何学习的感受，当下的失败只是不习惯算法思维，看不到对现实生活的任何实用性。 看了Tina最新一期关于如何快速学习编程的三个方法，觉得很有帮助，分享给大家： 直接学习：通过直接时间来学习你想学的东西，学习code应该是在了解一些基础知识后直接基于项目来coding 在学习过程中不要记太多的notes 在学习编程的初期，先学高级编程语言（比如python、javascript、java） 将复杂的问题横向纵向（分层）拆解，不要害怕麻烦（大工程→小工程） 学-练-用-造 学：读不懂也要读完，然后重复很多遍 绝不做预算不够的事情 参考书籍和文章 《学习之道》\n《刻意练习》\n《超速学习者》\n《在大脑外学习》\n《像运动员一样学习》\n《自学是门手艺》\n85% for learnning\n","date":"2022-09-12T00:00:00Z","permalink":"https://sandyness.github.io/p/%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0/","title":"如何学习"},{"content":"隨著深度學習的發展，自然語言處理 (Natural Language Processing，NLP) 技術越趨成熟，近年來在情感分析、機器翻譯、語音辨識、對話機器人等任務上均有很不錯的結果。\n自然語言處理是指讓電腦能夠分析、理解人類語言的一項技術，而人類語言具有前後順序、上下文關係，對於這種時間序列的資料很常使用循環神經網路 (Recurrent Neural Network，RNN)。但由於 RNN 難以進行平行運算，因此 Google 提出了一種不使用 RNN、CNN，僅使用自注意力機制 (self-attention mechanism) 的網路架構 — Transformer。\n本文將要介紹 Attention Is All You Need 論文，發表於 NIPS 2017。其網路架構是基於 Seq2Seq + self-attention mechanism。在開始閱讀之前，建議先了解 RNN、Seq2Seq 及注意力機制 (attention mechanism)，本文僅大略地介紹。\nSeq2Seq Seq2Seq主要由兩篇論文提出：Sequence to Sequence Learning with Neural Networks、Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation，兩者概念相同，差別在於使用不同的 RNN 模型。前者使用 LSTM，而後者使用 GRU。\n其架構為 Encoder-Decoder，如下圖所示，Encoder 會先將輸入句子進行編碼，得到的狀態會傳給 Decoder 解碼生成目標句子。\n注意力机制 但是當訊息太長時，seq2seq 容易丟失訊息，因此引入了注意力機制 (Attention Mechanism)。其概念為將 Encoder 所有資訊都傳給 Decoder，讓 Decoder 決定把注意力放在哪些資訊上。有兩篇經典的代表論文：Neural Machine Translation by Jointly Learning to Align and Translate、Effective Approaches to Attention-based Neural Machine Translation。\n编码器 先來看左邊的 Encoder 部分，input 先經過 embedding 層轉換為一個向量，然後在進入 layer 前會先與 Positional Encoding 相加 (Input Embedding 跟 Positional Encoding 的維度相等)，這個 Positional Encoding 就是詞語的位置編碼，目的是為了讓模型考慮詞語之間的順序。\n論文使用 sin, cos 函數進行位置編碼，公式如下，其中 pos 為詞語在序列中的位置、2i, 2i+1 為該詞語在 Positional Encoding 維度上的 index、d_model 為 Positional Encoding 的維度 (與 Input Embedding 維度相等)，預設值為 512。\n這樣講可能覺得有點難以理解，來舉個例子，假設要計算序列中的第二個詞語，此時 pos=1，則 Positional Encoding (PE) 為以下樣子\n❓ 為什麼要使用 sin, cos 函數進行編碼?\n因為 sin, cos 函數可以表示為兩個向量間的線性關係，能夠呈現不同詞語之間的相對位置，並且不受限序列長度的限制，比較不會有重複的問題。\n资源 此外，sin, cos 函數有上下界 (落於 [0, 1] 之間)、穩定循環的性質。\n资源 ❓ 為什麼是與 Positional Encoding 相加，而不是 concat ?\n因為其實做相加得到的結果與 concat 是相同的。假設有一輸入序列 xi，其位置使用簡單的 one-hot encoding 表示為 pi=(0, …, 1, 0)，W_x, W_p 為其相對應的權重。\n將 W_x, W_p 合併為 W，xi, pi 合併為 X，而 W 與 X 的 Inner Product 可經由線性代數的性質，拆分為 W_x 跟 xi 的 Inner Product + W_p 跟 pi 的 Inner Product。因此可以得知 Input Embedding 與 Positional Encoding 相加所得到的結果跟兩者 concat 是一樣的。\n多头注意力\nInput Embedding 與 Positional Encoding 相加後會進入到 layer 裡，Encoder 有兩個子層 Multi-head Attention、Feed Forward。在介紹 Multi-head Attention 之前，先來說明自注意機制 (self-attention mechanism)\n自注意机制 自注意力機制有三個重要的參數 q, k, v，而這些參數是由 input xi 經過 embedding 層轉換為 ai，接著 ai 進入到 self-attention layer 會乘上三個不同的 matrix 所得到的。\nq (query): 是指當前的詞向量，用於對每個 key 做匹配程度的打分\nk (key): 是指序列中的所有詞向量\nv (value): 是指實際的序列內容\n由下圖可以看到 q1 會對每一個 k 做 Inner Product 得到 q, k 之間匹配的相似程度 α1, 1、α1, 2、…，然後做一系列的運算得到輸出，這些計算步驟稱為 Scaled Dot-Product Attention\n资源 缩放点积注意力 接著來看數學式子會更了解，由下列公式得知 q, k 會先做 Inner Product，得到的值是匹配的相似程度，除以 sqrt(dk) 後，再做 softmax 計算出 v 的權重，最後將該權重與 v 做加權運算，其中 q, k 維度都為 dk，v 維度為 dv。\n看到這裡可能會疑惑為什麼要除以 sqrt(dk)? 之所以這樣做的原因是為了避免當 dk (q, k 的維度) 太大時，q, k Inner Product 的值過大，softmax 計算的值落入飽和區而導致梯度不穩定。\n多头注意力 終於要來介紹 Multi-Head Attention 啦~ 其運算方式與 self-attention mechanism 相同，差異在於會先將 q, k, v 拆分成多個低維度的向量，由下圖可看到若假設 head=2，qi 會拆分成 qi,1、qi,2，接著繼續跟上述一樣的步驟，最後再把這些 head 輸出 concat 起來做一次線性計算。\n這樣的好處是能夠讓各個 head (q, k, v) 關注不同的資訊，有些關注 local、有些關注 global 資訊等。\n资源 由下圖可看到 q, k, v 會做 h 次的線性映射到低維度的向量，再進行 Scaled Dot-Product Attention，最後將其 concat、linear 得到輸出。\n以下是 Multi-Head 公式，其中 h=8、dk=dv=d_model/h\n添加和规范\n經過 Multi-head Attention 後會進入 Add \u0026amp; Norm 層，這一層是指 residual connection 及 layer normalization。前一層的輸出 Sublayer 會與原輸入 x 相加 (residual connection)，以減緩梯度消失的問題，然後再做 layer normalization。\n📚 层归一化与批量归一化\n在之前的文章有介紹過 Batch Normalization (BN)，其作法是在每一個 mini-batch 的 input feature 做 normalize，這樣的方式雖然在 CNN 上獲得了很好的效果，但仍然存在一些缺點：\n過於依賴 batch size，如果 batch size 太小，BN 的效果會明顯下降。 不太適用於時間序列，因為文本序列的長度通常不一致，強制對每個文本執行 BN 不大合理。 因此在 RNN 中較常使用 Layer Normalization (LN)，概念與 BN 類似，差別在於 LN 是對每一個樣本進行 normalize。由下圖可以很清楚的看出兩者的差異，其中每一行是指樣本，每一列是樣本特徵。\n资源 位置前馈网络（前馈）\n接著進入到 FFN 層，由下列公式可以看到輸入 x 先做線性運算後，送入 ReLU，再做一次線性運算。其中輸入輸出的維度 d_model=512，而中間層的維度 dff = 2048\n解码器 看到這裡已經理解 Encoder 的運算過程了，再來看右邊 Decoder 的部分吧！\nDecoder 與 Encoder 一樣會先跟 Positional Encoding 相加再進入 layer，不同的是 Decoder 有三個子層 Masked Multi-head Attention、Multi-head Attention、Feed Forward。此外，中間層 Multi-head Attention 的輸入 q 來自於本身前一層的輸出，而 k, v 則是來自於 Encoder 的輸出。\n蒙面多头注意力\n由於其他兩層跟 Encoder 大致相同，所以就跳過來介紹 Decoder 中才有的 Masked Multi-head Attention。\nTransformer 的 Mask 機制有兩種：Padding Mask、Sequence Mask\nPadding Mask 在 Encoder和 Decoder 中都有使用到，目的是為了限制每個輸入的長度要相同，對於較短的句子會將不足的部分補 0 Sequence Mask 只用於 Decoder，目的是為了防止模型看到未來的資訊，因此在超過當前時刻 t 的輸出會加上一個 mask，確保模型的預測只依賴小於當前時刻的輸出。 Sequence Mask 的做法是通過一個上三角矩陣來實現，將這些區域的值都設定為負無窮，如此一來這些元素經過 softmax 後都會變為 0 以達到 mask 的效果。\n链接 📝 Transformer论文\n","date":"2022-08-07T00:00:00Z","permalink":"https://sandyness.github.io/p/transformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/","title":"Transformer论文阅读"},{"content":"Model Generator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class Generator(nn.Module): \u0026#34;\u0026#34;\u0026#34; Input shape: (batch, in_dim) Output shape: (batch, 3, 64, 64) \u0026#34;\u0026#34;\u0026#34; def __init__(self, in_dim, feature_dim=64): super().__init__() #input: (batch, 100) self.l1 = nn.Sequential( nn.Linear(in_dim, feature_dim * 8 * 4 * 4, bias=False), nn.BatchNorm1d(feature_dim * 8 * 4 * 4), nn.ReLU() ) self.l2 = nn.Sequential( self.dconv_bn_relu(feature_dim * 8, feature_dim * 4), #(batch, feature_dim * 16, 8, 8) self.dconv_bn_relu(feature_dim * 4, feature_dim * 2), #(batch, feature_dim * 16, 16, 16) self.dconv_bn_relu(feature_dim * 2, feature_dim), #(batch, feature_dim * 16, 32, 32) ) self.l3 = nn.Sequential( nn.ConvTranspose2d(feature_dim, 3, kernel_size=5, stride=2, padding=2, output_padding=1, bias=False), nn.Tanh() ) self.apply(weights_init) def dconv_bn_relu(self, in_dim, out_dim): return nn.Sequential( nn.ConvTranspose2d(in_dim, out_dim, kernel_size=5, stride=2, padding=2, output_padding=1, bias=False), #double height and width nn.BatchNorm2d(out_dim), nn.ReLU(True) ) def forward(self, x): y = self.l1(x) y = y.view(y.size(0), -1, 4, 4) y = self.l2(y) y = self.l3(y) return y Discriminator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class Discriminator(nn.Module): \u0026#34;\u0026#34;\u0026#34; Input shape: (batch, 3, 64, 64) Output shape: (batch) \u0026#34;\u0026#34;\u0026#34; def __init__(self, in_dim, feature_dim=64): super(Discriminator, self).__init__() #input: (batch, 3, 64, 64) \u0026#34;\u0026#34;\u0026#34; NOTE FOR SETTING DISCRIMINATOR: Remove last sigmoid layer for WGAN \u0026#34;\u0026#34;\u0026#34; self.l1 = nn.Sequential( nn.Conv2d(in_dim, feature_dim, kernel_size=4, stride=2, padding=1), #(batch, 3, 32, 32) nn.LeakyReLU(0.2), self.conv_bn_lrelu(feature_dim, feature_dim * 2), #(batch, 3, 16, 16) self.conv_bn_lrelu(feature_dim * 2, feature_dim * 4), #(batch, 3, 8, 8) self.conv_bn_lrelu(feature_dim * 4, feature_dim * 8), #(batch, 3, 4, 4) nn.Conv2d(feature_dim * 8, 1, kernel_size=4, stride=1, padding=0), nn.Sigmoid() ) self.apply(weights_init) def conv_bn_lrelu(self, in_dim, out_dim): \u0026#34;\u0026#34;\u0026#34; NOTE FOR SETTING DISCRIMINATOR: You can\u0026#39;t use nn.Batchnorm for WGAN-GP Use nn.InstanceNorm2d instead \u0026#34;\u0026#34;\u0026#34; return nn.Sequential( nn.Conv2d(in_dim, out_dim, 4, 2, 1), nn.BatchNorm2d(out_dim), nn.LeakyReLU(0.2), ) def forward(self, x): y = self.l1(x) y = y.view(-1) return y ","date":"2021-08-08T00:00:00Z","permalink":"https://sandyness.github.io/p/generative-adversarial-network/","title":"Generative Adversarial Network"}]