[{"content":"一、什么是学习？为什么学习？ 学习是由好奇心驱动的，学习的目的是帮助我们解决实际的问题。\n想要达成你的学习目标，不要想着如何更努力，而是找到做正确的方法，更聪明的学习。掌握「更有效率和效果」的学习计划、学习方法、和学习策略。天赋和智力不是必要条件。\n很多事情只是你现在「还不擅长」而已。重要的是「成长心态」，无论你对那一个学科的天赋如何，只要懂得投入时间和精力，加上适当指导，便能精通任何领域。特定的方式不是必要条件。每个人都有适合自己的学习方式。特定的动机不是必要条件。如果你想等到灵光乍现的动机出现，才开始学习的话，你可能会在等待中错过很多事。重点在于「自信」，你相信自己有能力达成目标、克服困难，相信自己能够学会任何一项你真正在乎的事情。\n学习的「总时间」是次要因素，学习的「高品质」才是主要因素。宏观计划与你把时间花在学习的「原因」有关，是为了确保目标是你真正想要的。在这个阶段非常重要，你要知道自己的总体学习目标和目的，确保即将花费的学习时间，可以创造出你想要的结果。\n二、认识大脑是如何学习的 大脑的结构和运行都会应对各种不同的心理训练而改变。 由训练引起的认知和生理变化需要继续保持，如果停止训练，它们便逐渐消失。 对于刻意练习，不仅要发掘自己的潜能，还要构筑它。 专注模式和发散模式 专注的练习和重复是创造记忆痕迹。在专注模式下工作，就像是为砌墙提供砖块，发散模式则是用泥浆把砖块逐渐结合在一起。 发散模式让学习更有深度和创造力。不断增加挫败感是个有益的暂停信号，暗示应该转到发散模式了。 工作记忆和长期记忆 工作记忆：在大脑中对正在处理的信息进行瞬时以及有意识加工的这部分记忆。只能一次扔四个球的杂耍演员。 长期记忆：可以看作仓库。东西一旦存进去，它们通常就一直待在那了。在学习数学和科学方面也很重要，解题时需要的基本概念和技巧都存储在那里。能存储大量知识的仓库，要靠定期回访保持对其中内容的新鲜感。 三、学习如何学习 为什么学？\n工具性动机 内在性动机 你到底要学什么？\n概念 事实 程序 制定目标和计划\n你跟目标之间的知识落差 达成目标的方式 画出你的学习蓝图 制定可衡量的目标 反思和回顾学习成效 目标具体明确，计划的可执行性，可量化且难度适中 专注\n**重复：**间隔复习所学的内容，对问题形成组块，让每一重复都有所提升\n回想：在学过一个概念和科学知识后，先在脑袋中进行回想练习，然后再学习再进行回想练习。 自我测试 输出：向他人讲解（费曼学习法），输出是最好的应用 及时反馈并自我纠正，创建有反馈的训练工具\n找位好导师 随时监测并追踪你的提高方法并保持动机 穿插学习法\n在练习中交替使用不同的解题技巧\n走出舒适区\n做之前没做过的事情，而不是更难的事情。\n睡眠\n睡眠时记忆和学习的重要环节。清除琐碎的记忆，增强重要部分。提升解决难题，理解知识的能力。让精力充沛的大脑阅读一个小时，强过疲惫的大脑读上三个小时。\n不要拖延\n困难的事情最先做\n大脑外学习\n关系：通过教别人让自己更好的思考 环境： 在能自主创造的环境里工作效率最高，用大屏幕有助于记忆 设计你的环境 练习在不同场所学习，不同的环境可以帮助换个角度看问题。 身体：手势的重要性 六、案例：我是如何学习数据科学的 设定目标 精通数据python 精通统计学 精通机器学习的各种模型和算法 制定计划 周计划(60h~70h) two topic/week 日计划（10h) study two things a day(技术 \u0026amp; 非技术) main topic you focus，one topic review give each topic at least a week repeat all the topic at least once 停止衡量你对任何学习的感受，当下的失败只是不习惯算法思维，看不到对现实生活的任何实用性。 看了Tina最新一期关于如何快速学习编程的三个方法，觉得很有帮助，分享给大家： 直接学习：通过直接时间来学习你想学的东西，学习code应该是在了解一些基础知识后直接基于项目来coding 在学习过程中不要记太多的notes 在学习编程的初期，先学高级编程语言（比如python、javascript、java） 将复杂的问题横向纵向（分层）拆解，不要害怕麻烦（大工程→小工程） 学-练-用-造 学：读不懂也要读完，然后重复很多遍 绝不做预算不够的事情 六、参考书籍和文章 《学习之道》\n《刻意练习》\n《超速学习者》\n《在大脑外学习》\n《像运动员一样学习》\n《自学是门手艺》\n85% for learnning\n","date":"2022-09-12T00:00:00Z","permalink":"https://sandyness.github.io/p/%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0/","title":"如何学习"},{"content":"Part 1: Sequence to Sequence Learning and Attention 论文“Attention Is All You Need”描述了转换器和所谓的序列到序列架构。Seq2Seq是一种神经网络，它将给定的元素序列（例如句子中的单词序列）转换为另一个序列。（好吧，考虑到这个名字，这可能不会让你感到惊讶。）Seq2Seq 模型特别擅长翻译，将一种语言的单词序列转换为另一种语言的不同单词序列。此类模型的一个流行选择是基于长短期记忆 (LSTM) 的模型。对于依赖于序列的数据，LSTM 模块可以赋予序列意义，同时记住（或忘记）它认为重要（或不重要）的部分。例如，句子是依赖于顺序的，因为单词的顺序对于理解句子至关重要。LSTM 是此类数据的自然选择。\nSeq2Seq 模型由一个 Encoder 和一个 Decoder 组成。Encoder 获取输入序列并将其映射到更高维空间（n 维向量）。该抽象向量被输入解码器，解码器将其转换为输出序列。输出序列可以是另一种语言、符号、输入的副本等。\n将编码器和解码器想象成只能说两种语言的人工翻译。他们的第一语言是他们的母语，这在他们两者之间是不同的（例如德语和法语），而他们的第二语言是他们共同的想象语言。为了将德语翻译成法语，编码器将德语句子转换成它所知道的另一种语言，即想象的语言。由于解码器能够读取该想象中的语言，它现在可以从该语言翻译成法语。该模型（由编码器和解码器组成）一起可以将德语翻译成法语！\n假设一开始，编码器和解码器都不是很流利的想象语言。为了学习它，我们在很多例子上训练他们（模型）。Seq2Seq 模型的编码器和解码器的一个非常基本的选择是它们每个都使用一个 LSTM。\n你想知道变形金刚什么时候最终会发挥作用，不是吗？\n我们还需要一个技术细节来让 Transformer 更容易理解：注意力。注意力机制查看输入序列，并在每个步骤中决定序列的哪些其他部分是重要的。这听起来很抽象，但让我用一个简单的例子来澄清一下：阅读这篇文章时，你总是把注意力集中在你读过的单词上，但同时你的大脑仍然会记住文本中的重要关键词，以便提供上下文。\n对于给定的序列，注意力机制的工作方式类似。对于我们的人类编码器和解码器的例子，想象一下，编码器不仅用想象的语言写下句子的翻译，还写下对句子语义很重要的关键字，并将它们提供给解码器除了常规翻译。这些新关键字使解码器的翻译变得更加容易，因为它知道句子的哪些部分是重要的，哪些关键术语给出了句子的上下文。\n换句话说，对于 LSTM（编码器）读取的每个输入，注意力机制会同时考虑几个其他输入，并通过为这些输入赋予不同的权重来决定哪些是重要的。然后，解码器将编码的句子和注意力机制提供的权重作为输入。要了解有关注意力的更多信息，请参阅这篇文章。对于比所提供的更科学的方法，请阅读这篇名为“基于注意力的神经机器翻译的有效方法”的伟大论文中关于序列到序列模型的不同基于注意力的方法。\n","date":"2022-08-07T00:00:00Z","permalink":"https://sandyness.github.io/p/transformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/","title":"Transformer论文阅读"},{"content":"Model Generator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class Generator(nn.Module): \u0026#34;\u0026#34;\u0026#34; Input shape: (batch, in_dim) Output shape: (batch, 3, 64, 64) \u0026#34;\u0026#34;\u0026#34; def __init__(self, in_dim, feature_dim=64): super().__init__() #input: (batch, 100) self.l1 = nn.Sequential( nn.Linear(in_dim, feature_dim * 8 * 4 * 4, bias=False), nn.BatchNorm1d(feature_dim * 8 * 4 * 4), nn.ReLU() ) self.l2 = nn.Sequential( self.dconv_bn_relu(feature_dim * 8, feature_dim * 4), #(batch, feature_dim * 16, 8, 8) self.dconv_bn_relu(feature_dim * 4, feature_dim * 2), #(batch, feature_dim * 16, 16, 16) self.dconv_bn_relu(feature_dim * 2, feature_dim), #(batch, feature_dim * 16, 32, 32) ) self.l3 = nn.Sequential( nn.ConvTranspose2d(feature_dim, 3, kernel_size=5, stride=2, padding=2, output_padding=1, bias=False), nn.Tanh() ) self.apply(weights_init) def dconv_bn_relu(self, in_dim, out_dim): return nn.Sequential( nn.ConvTranspose2d(in_dim, out_dim, kernel_size=5, stride=2, padding=2, output_padding=1, bias=False), #double height and width nn.BatchNorm2d(out_dim), nn.ReLU(True) ) def forward(self, x): y = self.l1(x) y = y.view(y.size(0), -1, 4, 4) y = self.l2(y) y = self.l3(y) return y Discriminator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class Discriminator(nn.Module): \u0026#34;\u0026#34;\u0026#34; Input shape: (batch, 3, 64, 64) Output shape: (batch) \u0026#34;\u0026#34;\u0026#34; def __init__(self, in_dim, feature_dim=64): super(Discriminator, self).__init__() #input: (batch, 3, 64, 64) \u0026#34;\u0026#34;\u0026#34; NOTE FOR SETTING DISCRIMINATOR: Remove last sigmoid layer for WGAN \u0026#34;\u0026#34;\u0026#34; self.l1 = nn.Sequential( nn.Conv2d(in_dim, feature_dim, kernel_size=4, stride=2, padding=1), #(batch, 3, 32, 32) nn.LeakyReLU(0.2), self.conv_bn_lrelu(feature_dim, feature_dim * 2), #(batch, 3, 16, 16) self.conv_bn_lrelu(feature_dim * 2, feature_dim * 4), #(batch, 3, 8, 8) self.conv_bn_lrelu(feature_dim * 4, feature_dim * 8), #(batch, 3, 4, 4) nn.Conv2d(feature_dim * 8, 1, kernel_size=4, stride=1, padding=0), nn.Sigmoid() ) self.apply(weights_init) def conv_bn_lrelu(self, in_dim, out_dim): \u0026#34;\u0026#34;\u0026#34; NOTE FOR SETTING DISCRIMINATOR: You can\u0026#39;t use nn.Batchnorm for WGAN-GP Use nn.InstanceNorm2d instead \u0026#34;\u0026#34;\u0026#34; return nn.Sequential( nn.Conv2d(in_dim, out_dim, 4, 2, 1), nn.BatchNorm2d(out_dim), nn.LeakyReLU(0.2), ) def forward(self, x): y = self.l1(x) y = y.view(-1) return y ","date":"2021-08-08T00:00:00Z","permalink":"https://sandyness.github.io/p/generative-adversarial-network/","title":"Generative Adversarial Network"},{"content":"Model Generator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class Generator(nn.Module): \u0026#34;\u0026#34;\u0026#34; Input shape: (batch, in_dim) Output shape: (batch, 3, 64, 64) \u0026#34;\u0026#34;\u0026#34; def __init__(self, in_dim, feature_dim=64): super().__init__() #input: (batch, 100) self.l1 = nn.Sequential( nn.Linear(in_dim, feature_dim * 8 * 4 * 4, bias=False), nn.BatchNorm1d(feature_dim * 8 * 4 * 4), nn.ReLU() ) self.l2 = nn.Sequential( self.dconv_bn_relu(feature_dim * 8, feature_dim * 4), #(batch, feature_dim * 16, 8, 8) self.dconv_bn_relu(feature_dim * 4, feature_dim * 2), #(batch, feature_dim * 16, 16, 16) self.dconv_bn_relu(feature_dim * 2, feature_dim), #(batch, feature_dim * 16, 32, 32) ) self.l3 = nn.Sequential( nn.ConvTranspose2d(feature_dim, 3, kernel_size=5, stride=2, padding=2, output_padding=1, bias=False), nn.Tanh() ) self.apply(weights_init) def dconv_bn_relu(self, in_dim, out_dim): return nn.Sequential( nn.ConvTranspose2d(in_dim, out_dim, kernel_size=5, stride=2, padding=2, output_padding=1, bias=False), #double height and width nn.BatchNorm2d(out_dim), nn.ReLU(True) ) def forward(self, x): y = self.l1(x) y = y.view(y.size(0), -1, 4, 4) y = self.l2(y) y = self.l3(y) return y Discriminator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class Discriminator(nn.Module): \u0026#34;\u0026#34;\u0026#34; Input shape: (batch, 3, 64, 64) Output shape: (batch) \u0026#34;\u0026#34;\u0026#34; def __init__(self, in_dim, feature_dim=64): super(Discriminator, self).__init__() #input: (batch, 3, 64, 64) \u0026#34;\u0026#34;\u0026#34; NOTE FOR SETTING DISCRIMINATOR: Remove last sigmoid layer for WGAN \u0026#34;\u0026#34;\u0026#34; self.l1 = nn.Sequential( nn.Conv2d(in_dim, feature_dim, kernel_size=4, stride=2, padding=1), #(batch, 3, 32, 32) nn.LeakyReLU(0.2), self.conv_bn_lrelu(feature_dim, feature_dim * 2), #(batch, 3, 16, 16) self.conv_bn_lrelu(feature_dim * 2, feature_dim * 4), #(batch, 3, 8, 8) self.conv_bn_lrelu(feature_dim * 4, feature_dim * 8), #(batch, 3, 4, 4) nn.Conv2d(feature_dim * 8, 1, kernel_size=4, stride=1, padding=0), nn.Sigmoid() ) self.apply(weights_init) def conv_bn_lrelu(self, in_dim, out_dim): \u0026#34;\u0026#34;\u0026#34; NOTE FOR SETTING DISCRIMINATOR: You can\u0026#39;t use nn.Batchnorm for WGAN-GP Use nn.InstanceNorm2d instead \u0026#34;\u0026#34;\u0026#34; return nn.Sequential( nn.Conv2d(in_dim, out_dim, 4, 2, 1), nn.BatchNorm2d(out_dim), nn.LeakyReLU(0.2), ) def forward(self, x): y = self.l1(x) y = y.view(-1) return y ","date":"2021-08-08T00:00:00Z","permalink":"https://sandyness.github.io/p/generative-adversarial-network/","title":"Generative Adversarial Network"}]