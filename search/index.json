[{"content":"本文介绍CNN的经典模型ResNet。ResNet在2015年微软亚洲院的何凯明博士提出，并在同年的ImageNet LSVRC竞赛中获得了冠军，同时也在ImageNet detection、 ImageNet localization、COCO detection和COCO segmentation等任务中获得第一名，此外还获得了CVPR2016最佳论文奖。 ResNet模型一共使用了152层，其深度比 GoogLeNet 高了七倍多 (20层)，并且模型在ImageNet测试的错误率为 3.6%，人类在 ImageNet 的错误率为 5.1%，模型的错误率已经达到比人类的错误率还要小的程度。\n解决的问题 对很多计算机视觉的任务来说，模型的深度是十分重要的，但是深度模型又难以训练。因为随着模型层数增加到一定程度时，反而会降低训练的准确率。但并不是过拟合，而是深层网络退化的问题。\n网络退化是指层数越来越大时，梯度在做反向传播的过程中消失，导致梯度无法对参数进行跟新。而ResNet为了解决这一问题， 提出了 Residual Learning 让深层网络更容易训练。\n模型架构 什么是Residual Learning？\nResidual Learning 使用 Shortcut Connection 的结构。该结构有两个分支：一个是将输入的 x 跨层传递，另一个是 F(x)。将这两个分支相加再送入激活函数中。输出值为 H(x) = F(x) + x。 当训练达到饱和的时候，F(x) 容易被优化成 0，这时只剩下 x, 此时 H(x) = x，这种情况是 Identify mapping。 什么是Residual Block？\nResidual 采用模组化的方式，对不同的 ResNet，作者提出了两种 Residual Block。\n左图为基本的 Residual Block，使用连续的两个 3 x 3卷积层，并每两个卷积层进行一次 Shortcut Connection，用于 ResNet-34。\n右图则是针对较深的网络所做的改进，称为 bottleneck。因为模型的层数越多，导致训练成本变高，因此为了降低维度，再送入 3 x 3 卷积层之前，先通过 1 x 1 卷积层降低维度，最后再通过 1 x 1卷积 层恢复原本的维度，用于 ResNet-50、ResNet-101、ResNet-152。\n模型架构\n下图为 ResNet 不同层数的网络架构， 第一层为 7x7 的卷积层，接着是 3x3 Maxpooling，再使用大量的 Residual Block，最后使用平均池化 (Global Average Pooling) 并传入全连接层进行分类。 下图为 ResNet-34 的网络架构，虚线部分就是指在做 Shortcut Connection 时，輸入的 x 权重輸出的 F(x) 通道 (channel) 數目不同，因此需要使用1x1 卷积层调整通道维度，使其可以做相加的运算。 代码实现 ResNet 根据不同的深度有不同Residual Block，所以 code 有定义 basic_block 和 bottleneck_block 两组模块。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 class basic_block(nn.Module): # 输出通道的倍数 expansion = 1 def __init__(self, in_channels, out_channels, stride, downsample): super(basic_block, self).__init__() self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=stride, padding=1, bias=False) self.bn1 = nn.BatchNorm2d(out_channels) self.relu = nn.ReLU(inplace=True) self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1, bias=False) self.bn2 = nn.BatchNorm2d(out_channels) # 在 shortcut 时，若维度不一样，则要修改维度 self.downsample = downsample def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample is not None: residual = self.downsample(x) out += residual out = self.relu(out) return out class bottleneck_block(nn.Module): # 输出通道的倍数 expansion = 4 def __init__(self, in_channels, out_channels, stride, downsample): super(bottleneck_block, self).__init__() self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, bias=False) self.bn1 = nn.BatchNorm2d(out_channels) self.relu = nn.ReLU(inplace=True) self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=stride, padding=1, bias=False) self.bn2 = nn.BatchNorm2d(out_channels) self.conv3 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels * self.expansion, kernel_size=1, bias=False) self.bn3 = nn.BatchNorm2d(out_channels * self.expansion) # 在 shortcut 时，若维度不一样，则要修改维度 self.downsample = downsample def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.relu(out) out = self.conv3(out) out = self.bn3(out) if self.downsample is not None: residual = self.downsample(x) out += residual out = self.relu(out) return out class ResNet(nn.Module): def __init__(self, net_block, layers, num_classes): super(ResNet, self).__init__() self.in_channels = 64 self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False) self.bn1 = nn.BatchNorm2d(64) self.relu = nn.ReLU(inplace=True) self.maxpooling = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) self.layer1 = self.net_block_layer(net_block, 64, layers[0]) self.layer2 = self.net_block_layer(net_block, 128, layers[1], stride=2) self.layer3 = self.net_block_layer(net_block, 256, layers[2], stride=2) self.layer4 = self.net_block_layer(net_block, 512, layers[3], stride=2) self.avgpooling = nn.AvgPool2d(7, stride=1) self.fc = nn.Linear(512 * net_block.expansion, num_classes) # 参数初始化 for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode=\u0026#34;fan_out\u0026#34;, nonlinearity=\u0026#34;relu\u0026#34;) elif isinstance(m, nn.BatchNorm2d): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) def net_block_layer(self, net_block, out_channels, num_blocks, stride=1): downsample = None # 在 shortcut 时，若维度不一样，则要修改维度 if stride != 1 or self.in_channels != out_channels * net_block.expansion: downsample = nn.Sequential(nn.Conv2d(self.in_channels, out_channels * net_block.expansion, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(out_channels * net_block.expansion)) layers = [] layers.append(net_block(self.in_channels, out_channels, stride, downsample)) if net_block.expansion != 1: self.in_channels = out_channels * net_block.expansion else: self.in_channels = out_channels for i in range(1, num_blocks): layers.append(net_block(self.in_channels, out_channels, 1, None)) return nn.Sequential(*layers) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.maxpooling(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) x = self.avgpooling(x) x = torch.flatten(x, start_dim=1) x = self.fc(x) return x def ResNet_n(num_layers): if num_layers == 18: # ResNet18 model = ResNet(basic_block, [2, 2, 2, 2], num_classes) elif num_layers == 34: # ResNet34 model = ResNet(basic_block, [3, 4, 6, 3], num_classes) elif num_layers == 50: # ResNet50 model = ResNet(bottleneck_block, [3, 4, 6, 3], num_classes) elif num_layers == 101: # ResNet101 model = ResNet(bottleneck_block, [3, 4, 23, 3], num_classes) elif num_layers == 152: # ResNet152 model = ResNet(bottleneck_block, [3, 8, 36, 3], num_classes) else: print(\u0026#34;error\u0026#34;) return return model 参考链接 论文链接 ResNet【论文精读】 ","date":"2021-08-08T00:00:00Z","permalink":"https://sandyness.github.io/p/cnnresnet/","title":"CNN｜ResNet"},{"content":"随着深度学习的发展，自然语言处理（NLP）技术越来越成熟。在机器翻译、语音识别、情感分析、文本生成、人机对话等任务上都有很好的结果。但由于RNN难以进行并行计算，因此Google提出了一种不使用RNN和CNN，而是使用叫自注意力机制(self-attention mechanism)-Transformer。其网络架构是基于 Seq2Seq + self-attention mechanism。\nSeq2Seq\nSeq2Seq主要由两篇论文提出：Sequence to Sequence Learning with Neural Networks、 Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation。两者差别是前者使用LSTM，后者使用GRU。\n如下图所示，其架构为 Encoder-Decoder，Encoder 先将输入句子进行编码，得到的状态传给 Decoder 解码生成目标句子。 注意力机制\n但是当讯息太长时，seq2seq容易丢失讯息，因此引入了注意力机制。其概念为将Encoder所有资讯传给Decoder，让Decoder决定将注意力放在哪些资讯上。 有两篇论文：Neural Machine Translation by Jointly Learning to Align and Translate 和 Effective Approaches to Attention-based Neural Machine Translation。 Transformer 模型架构 Transformer 由 N=6 的 Encoder-Decoder 堆叠而成。Encoder-Decoder 结构有 Multi-head Attention、Add\u0026amp;Norm、Feed Forward、Masked Multi-head Attention。 Encoder Positional Encoding（位置编码） input 先经过 embedding 层转换为一个向量，然后在进入 layer 前先与 Positional Encoding 相加。Positional Encoding 是词语的位置编码，目的是为了让模型考虑词语之间的顺序。\n论文只用 sin, cos 函数进行位置编码，公式如下：\n其中 pos 为词语在序列中的位置、2i, 2i+1 为词语在 Positional Encoding 维度上的 index、d_model 为 Positional Encoding 的维度 (与 Input Embedding 维度相等)，预设值512。\nQ: 为什么要使用 sin, cos 函数进行位置编码?\nsin, cos 函数可以表示两个向量间的线性关系，能呈现不同词语之间的相对位置，并且不受序列长度的限制，不会有重复的问题。\n此外，sin, cos 函数有上下界 (位于 [0, 1] 之间)、具有稳定循环的性质。 多头注意力机制 Input Embedding 和 Positional Encoding 相加后进入到 layer 里。Encoder 有 Multi-head Attention 和 Feed Forward 这两个子层。 自注意机制\n自注意力机制三个重要的参数 q, k, v。\nq (query): 是指当前的词向量，用于对每个 key 做匹配程度的打分；\nk (key): 是指序列中的所有词向量；\nv (value): 是指实际的序列內容。\n由下图可以看到q1会对每个k做内积得到的q, k之间匹配的相似程度a1,1、a1,2\u0026hellip;,然后做一些列运算得到输出，这些计算步骤称为 Scaled Dot-Product Attention。\nScaled Dot-Product Attention\n由下列公式得知 q, k 会先做 Inner Product，得到的值是匹配的相似程度，除以 sqrt(dk) 后，再做 softmax 计算出 v 的权重， 最后与 v 做加权运算，其中 q, k 维度都为 dk，v 维度wei dv。除以 sqrt(dk)是为了避免当 dk (q, k 的维度) 太大时，q, k Inner Product 的值过大，softmax 计算的值落入饱和区而导致梯度不稳定。 多头注意力\n其运算方式与 self-attention mechanism 相同。区别是先将q,k,v拆分成多个低维度的向量。若假设head=2,qi会拆分为qi,1、qi,2。最后再把这些head输出concat起来做一次线性运算。\n这样的好处是能够让各个head(q,k,v)关注不同的资讯，有些关注local，有些关注global资讯。 由下图可以看到q,k,v会做h次的线性映射到低维度的向量，再进行 Scaled Dot-Product Attention，最后将其concat、linear 得到输出。 Add \u0026amp; Norm 经过 Multi-head Attention 后进入 Add \u0026amp; Norm 层，这一层是指 residual connection 和 layer normalization。前一层的输出 Sublayer 与输入 x 相加 (residual connection)，减缓梯度消失的问题，然后再做 layer normalization。 Feed-Forward Networks (FNN) 公式如下： 输入 x 先做线性运算，送入 ReLU，再做一次线性运算。其中输入输出的为度 d_model=512，中间层的维度 dff = 2048。\nDecoder Decoder 与 Encoder 一样先 Positional Encoding 相加再进入 layer，区别是 Decoder 有 Masked Multi-head Attention、Multi-head Attention、Feed Forward 这三个子层。 此外， Multi-head Attention 的输入 q 來自自身前一层的输出，而 k, v 则是来自于 Encoder 的输出。 Masked Multi-head Attention Transformer 的 掩码机制有两种：Padding Mask、Sequence Mask\nPadding Mask 在 Encoder 和 Decoder 中都有使用到，目的是为了限制每个输入的长度要相同，对较短的句子会將不足的部分补 0；\nSequence Mask 只用于 Decoder，目的是为了防止模型看到未来的资讯，因此在超过当前时刻t的输出会加一个mask，确保模型的预测只依赖于当前时刻的输出。\nSequence Mask 的做法是通过一个上三角矩阵，将这些区域的值都设为负无穷，这样这些元素经过softmax后都会变为0达到掩码的效果。\n参考链接 论文链接 李沐老师的Transformer【论文精读】 ","date":"2021-07-07T00:00:00Z","permalink":"https://sandyness.github.io/p/transformer/","title":"Transformer"},{"content":"AlexNet 是 Alex Krizhevsky 于2012提出的。并在同年的 ImageNet LSVRC 竞赛中获得了冠军。AlexNet 架构有8层，其中5个卷积层、3个全连接层。\n模型架构 第1层到第5层是卷积层。其中第1、2、5卷积层使用size为3 x 3、stride为2的Maxpooling，因为 stride(2) \u0026lt; size(3), pooling 可以重叠，所以能够重复检视特征，避免重要的特征被舍弃掉。还能够避免过拟合。第6层到第8层使用全连接层。\nInput size\n将输入尺寸变大，可以输入 224 x 224 尺寸的彩色图片。\nKernel size\n由于输入层变大，所以将第一层的 kernel 设定为 11 x 11, stride为4，第二层则使用 size为5 x 5的 kernel, 之后的层都使用size为3 x 3 ，stride 为1的 kernel。\nActive function\n使用 ReLU 作为激活函数，避免因为神经网络层数过深或梯度过小，导致梯度消失(Vanishing gradient) 的问题。相比 Sigmoid/Tanh，ReLU收敛速度更快。\n避免过拟合的方法\nDropout\n是指在每一次训练过程中，随机选取神经元使其不参与训练。可以让模型不过度依赖某些特征，增强模型的泛化能力。 AlexNet在第6、7层的全连接层中加入了Dropout,设定值为0.5。\n图像增强\n使用随机裁剪、水平变换、颜色变化的方法进行了数据增强。\n将大小为256 x 256的图片进行随机裁剪大小为224 x 224，再进行水平翻转； 对RGB通道做主成分分析（PCA），再使用高斯分布，对颜色和亮度进行变换。 GPU\n因为单个GTX580 GPU无法负荷大数据量的运算，AlexNet使用了GTX580 GPU 进行同步训练，加快了训练速度。\n代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class AlexNet(nn.Module): def __init__(self, num_classes): super(AlexNet, self).__init__() self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=11, padding=2, stride=4) self.conv2 = nn.Conv2d(in_channels=64, out_channels=192, kernel_size=5, padding=2) self.conv3 = nn.Conv2d(in_channels=192, out_channels=384, kernel_size=3, padding=1) self.conv4 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1) self.conv5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1) self.fc1 = nn.Linear(in_features=256*6*6, out_features=4096) self.fc2 = nn.Linear(in_features=4096, out_features=1024) self.fc3 = nn.Linear(in_features=1024, out_features=num_classes) def forward(self, x): x = F.relu(self.conv1(x)) x = F.max_pool2d(x, kernel_size=3, stride=2) x = F.relu(self.conv2(x)) x = F.max_pool2d(x, kernel_size=3, stride=2) x = F.relu(self.conv3(x)) x = F.relu(self.conv4(x)) x = F.relu(self.conv5(x)) x = F.max_pool2d(x, kernel_size=3, stride=2) # x = x.view(x.size(0), -1) x = torch.flatten(x, start_dim=1) x = F.relu(self.fc1(x)) x = F.dropout(x, p=0.5) x = F.relu(self.fc2(x)) x = F.dropout(x, p=0.5) x = self.fc3(x) return x 参考链接 论文链接 AlexNet【论文精读】 ","date":"2021-05-05T00:00:00Z","permalink":"https://sandyness.github.io/p/cnnalexnet/","title":"CNN｜AlexNet"},{"content":"什么是学习？为什么学习？ 学习是由好奇心驱动的，学习的目的是帮助我们解决实际的问题。\n认识大脑是如何学习的 大脑的结构和运行都会应对各种不同的心理训练而改变。 由训练引起的认知和生理变化需要继续保持，如果停止训练，它们便逐渐消失。 专注模式和发散模式 专注模式：专注的练习和重复是创造记忆痕迹。在专注模式下工作，就像是为砌墙提供砖块。 发散模式：发散模式则是用泥浆把砖块逐渐结合在一起。让学习更有深度和创造力。不断增加挫败感是个有益的暂停信号，暗示应该转到发散模式了。 工作记忆和长期记忆 工作记忆：在大脑中对正在处理的信息进行瞬时以及有意识加工的这部分记忆。只能一次扔四个球的杂耍演员。 长期记忆：可以看作仓库。东西一旦存进去，它们通常就一直待在那了。在学习数学和科学方面也很重要，解题时需要的基本概念和技巧都存储在那里。能存储大量知识的仓库，要靠定期回访保持对其中内容的新鲜感。 学习如何学习 想要达成你的学习目标，不要想着如何更努力，而是找到做正确的方法，更聪明的学习。掌握「更有效率和效果」的学习计划、学习方法、和学习策略。天赋和智力不是必要条件。\n很多事情只是你现在「还不擅长」而已。重要的是「成长心态」，无论你对那一个学科的天赋如何，只要懂得投入时间和精力，加上适当指导，便能精通任何领域。特定的方式不是必要条件。每个人都有适合自己的学习方式。特定的动机不是必要条件。如果你想等到灵光乍现的动机出现，才开始学习的话，你可能会在等待中错过很多事。重点在于「自信」，你相信自己有能力达成目标、克服困难，相信自己能够学会任何一项你真正在乎的事情。\n学习的「总时间」是次要因素，学习的「高品质」才是主要因素。宏观计划与你把时间花在学习的「原因」有关，是为了确保目标是你真正想要的。在这个阶段非常重要，要知道自己的总体学习目标和目的，确保即将花费的学习时间，可以创造出你想要的结果。\n为什么学？ 工具性动机 内在性动机 学什么？ 概念 事实 程序 制定目标和计划 你跟目标之间的知识落差 达成目标的方式 画出你的学习蓝图 制定可衡量的目标 反思和回顾学习成效 目标具体明确，计划的可执行性，可量化且难度适中 学习tips 专注 重复:间隔复习所学的内容，对问题形成组块，让每一重复都有所提升 回想：在学过一个概念和科学知识后，先在脑袋中进行回想练习，然后再学习再进行回想练习。 自我测试 输出：向他人讲解（费曼学习法），输出是最好的应用 及时反馈并自我纠正，创建有反馈的训练工具 找位好导师 随时监测并追踪你的提高方法并保持动机 走出舒适区：做之前没做过的事情，而不是更难的事情。 睡眠：睡眠时记忆和学习的重要环节。清除琐碎的记忆，增强重要部分。提升解决难题，理解知识的能力。让精力充沛的大脑阅读一个小时，强过疲惫的大脑读上三个小时。 不要拖延：困难的事情最先做 环境 在能自主创造的环境里工作效率最高，用大屏幕有助于记忆 练习在不同场所学习，不同的环境可以帮助换个角度看问题。 案例：如何学习数据科学 设定目标 精通数据python 精通统计学 精通机器学习的各种模型和算法 制定计划 周计划(60h~70h）：two topic/week 日计划（10h) study two things a day(技术 \u0026amp; 非技术) main topic you focus，one topic review give each topic at least a week repeat all the topic at least once 看了Tina最新一期关于如何快速学习编程的三个方法，觉得很有帮助，分享给大家： 直接学习：通过直接时间来学习你想学的东西，学习code应该是在了解一些基础知识后直接基于项目来coding 在学习过程中不要记太多的notes 在学习编程的初期，先学高级编程语言（比如python、javascript、java） 将复杂的问题横向纵向（分层）拆解，不要害怕麻烦（大工程→小工程） Reference 书\n《学习之道》 《刻意练习》 《超速学习者》 《在大脑外学习》 文章\n像运动员一样学习\n自学是门手艺\n85% for learning\n","date":"2021-04-02T00:00:00Z","permalink":"https://sandyness.github.io/p/%E6%95%88%E7%8E%87%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0/","title":"效率｜如何学习"},{"content":"特征工程，是对原始数据进行一系列的工程处理，去除原始数据的杂质和荣誉，将其提炼为可供机器学习的输入特征。机器学习常用的数据类型包括结构化数据和非结构化数据。\n结构化数据：可以看作关系型数据库的一张表，每列都有清晰的定义，包含了数值型、类别型两种基本类型;每一行数据表示一个样本的信息。 非结构化数据：主要包括文本、图像、音频、视频数据， 其包含的信息无法用一个简单的数值表示，也没有清晰的类别定义，并且每条数据的大小各不相同。 数值类型 特征归一化: 为了消除数据特征之间的量纲影响，需要对特征进行归一化(Normalization)处理，将所有的特征都统一到一个大致相同的数值区间内，在随机梯度下降的过程中， 特征的更新速度变得更为一致，容易更快地通过梯度下降找到最优解。\n最常用的方法主要有以下两种。\n线性函数归一化(Min-Max Scaling)：对原始数据进行线性变换，使结果映射到[0, 1]的范围，实现对原始数据的等比缩放。公式如下： X= (X - Xmax) / (Xmax - Xmin)\n1 from sklearn.preprocessing import MinMaxScaler 其中X为原始数据，Xmax、Xmin分别为数据最大值和最小值。\n零均值归一化(Z-Score Normalization)：它会将原始数据映射到均值为 0、标准差为1的分布上。若原始特征的均值为μ、标准差为σ,归一化公式定义为： z = (X - μ) / σ\n1 from sklearn.preprocessing import StandardScaler 在实际应用中，通过梯度下降法求解的模型通常是需要归一化的，包括线性回归、逻辑回归、支持向量机、神经网络等模型。 但对于决策树模型则并不适用，决策树在进行节点分裂时主要依据数据集和特征的信息增益比，而信息增益比跟特征是否经过归一化是无关的，因为归一化并不会改变样本在特征上的信息增益。\n类别类型 类别型特征(Categorical Feature)是只在有限选项内取值的特征，如性别(男、女)、血型(A、B、 AB、O)等。\n类别型特征原始输入通常是字符串形式，除了决策树等少数模型能直接处理字符串形式的输入，对于逻辑回归、支持向量机等模型来说，类别型特征必须经过处理转换成数值型特征才能正确工作。\n最常用的方法主要有以下两种。\n序号编码（Ordinal Encoding）: 常用来处理具有大小关系的类别特征（eg.成绩高中低）\n1 from sklearn.preprocessing import OrdinalEncoder 独热编码(One-hot Encoding）：不具有大小关系的类别特征（eg.血型）\n1 from sklearn.preprocessing import OneHotEncoder 在独热编码下，特征向量只有某一维取值为1，其他位置取值均为0。因此可以利用向量的稀疏表示有效地节省空间，并且目前大部分的算法均接受稀疏向量形式的输入。 高维度特征会带来几方面的问题。一是在K近邻算法中，高维空间下两点之间的距离很难得到有效的衡量;二是在逻辑回归模型中，参数的数量会随着维度的增高而增加，容易引起过拟合问题;三是通常 只有部分维度是对分类、预测有帮助，因此可以考虑配合特征选择来降低维度。 二进制编码(Binary Encoding)：利用二进制对ID进行哈希映射，最终得到0/1特征向量，且维数少于独热编 码，节省了存储空间。\n文本 Bag of words和N-gram模型\nBag of words 就是将每篇文章看成一袋子词。将整段文本以词为单位切分开， 然后每篇文章可以表示成一个长向量，向量中的每一维代表一个单词，而该维对应的权重则反映了这个词在原文章中的重要程度。 常用TF-IDF来计算权重，公式为TF-IDF(t,d)=TF(t,d)×IDF(t) ， 其中TF(t,d)为单词t在文档d中出现的频率，IDF(t)是逆文档频率，用来衡量单词t对表达语义所起的重要性。\nword2vec\n图片 图像数据不足带来的问题？\n图像数据不足时的处理方法？\n数据： 数据增强 GAN 模型： 简化模型 添加约束项缩小假设空间（l1 or l2正则项） 集成学习 dropout超参数设置 迁移学习：fine-tune已经训练好的模型 ","date":"2020-01-01T00:00:00Z","permalink":"https://sandyness.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/","title":"机器学习笔记（一） - 数据预处理"},{"content":"特征工程，是对原始数据进行一系列的工程处理，去除原始数据的杂质和荣誉，将其提炼为可供机器学习的输入特征。机器学习常用的数据类型包括结构化数据和非结构化数据。\n分类模型评估 准确率、精确率、召回率: F1值 ROC曲线 AUC曲线 P-R曲线 类别类型 类别型特征(Categorical Feature)是只在有限选项内取值的特征，如性别(男、女)、血型(A、B、 AB、O)等。\n类别型特征原始输入通常是字符串形式，除了决策树等少数模型能直接处理字符串形式的输入，对于逻辑回归、支持向量机等模型来说，类别型特征必须经过处理转换成数值型特征才能正确工作。\n最常用的方法主要有以下两种。\n序号编码（Ordinal Encoding）: 常用来处理具有大小关系的类别特征（eg.成绩高中低）\n1 from sklearn.preprocessing import OrdinalEncoder 独热编码(One-hot Encoding）：不具有大小关系的类别特征（eg.血型）\n1 from sklearn.preprocessing import OneHotEncoder 在独热编码下，特征向量只有某一维取值为1，其他位置取值均为0。因此可以利用向量的稀疏表示有效地节省空间，并且目前大部分的算法均接受稀疏向量形式的输入。 高维度特征会带来几方面的问题。一是在K近邻算法中，高维空间下两点之间的距离很难得到有效的衡量;二是在逻辑回归模型中，参数的数量会随着维度的增高而增加，容易引起过拟合问题;三是通常 只有部分维度是对分类、预测有帮助，因此可以考虑配合特征选择来降低维度。 二进制编码(Binary Encoding)：利用二进制对ID进行哈希映射，最终得到0/1特征向量，且维数少于独热编 码，节省了存储空间。\n文本 Bag of words和N-gram模型\nBag of words 就是将每篇文章看成一袋子词。将整段文本以词为单位切分开， 然后每篇文章可以表示成一个长向量，向量中的每一维代表一个单词，而该维对应的权重则反映了这个词在原文章中的重要程度。 常用T\n","date":"2020-01-01T00:00:00Z","permalink":"https://sandyness.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/","title":"机器学习笔记（一） - 模型评估"}]