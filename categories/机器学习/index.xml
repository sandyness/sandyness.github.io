<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>机器学习 on 一片生菜叶</title>
        <link>https://sandyness.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
        <description>Recent content in 机器学习 on 一片生菜叶</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 05 Jan 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://sandyness.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>机器学习笔记(三) - 集成学习（Ensemble Learning）</title>
        <link>https://sandyness.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%89-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0ensemble-learning/</link>
        <pubDate>Sun, 05 Jan 2020 00:00:00 +0000</pubDate>
        
        <guid>https://sandyness.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%89-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0ensemble-learning/</guid>
        <description>&lt;p&gt;传统机器学习算法 (例如：决策树，人工神经网络，支持向量机，朴素贝叶斯等) 的目标都是寻找一个最优分类器尽可能的将训练数据分开。
集成学习 (Ensemble Learning) 算法的基本思想就是将多个分类器组合，从而实现一个预测效果更好的集成分类器。
集成算法可以说从一方面验证了中国的一句老话：三个臭皮匠，赛过诸葛亮。&lt;/p&gt;
&lt;p&gt;集成算法大致可以分为：Bagging，Boosting 和 Stacking 等类型。&lt;/p&gt;
&lt;h2 id=&#34;bagging&#34;&gt;Bagging&lt;/h2&gt;
&lt;p&gt;Bagging (Boostrap Aggregating) 是由 Breiman 于 1996 年提出的，基本思想如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;每次采用有放回的抽样从训练集中取出n个训练样本组成新的训练集;&lt;/li&gt;
&lt;li&gt;利用新的训练集，训练得到 M 个子模型;&lt;/li&gt;
&lt;li&gt;对于分类问题，采用投票的方法，得票最多子模型的分类类别为最终的类别；对于回归问题，采用简单的平均方法得到预测值。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;random-forest&#34;&gt;Random Forest&lt;/h3&gt;
&lt;p&gt;随机森林 (Random Forests) 是一种利用决策树作为基学习器的 Bagging 集成学习算法。随机森林模型的构建过程如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;数据采样&lt;/p&gt;
&lt;p&gt;作为一种 Bagging 集成算法，随机森林同样采用有放回的采样，对于总体训练集T，抽样一个子集T(sub)作为训练样本集。
除此之外，假设训练集的特征个数为d，每次仅选择k(k &amp;lt; d)个特征构建决策树。
因此，随机森林除了能够做到样本扰动外，还添加了特征扰动。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;树的构建&lt;/p&gt;
&lt;p&gt;每次根据采样得到的数据和特征构建一棵决策树。在构建决策树的过程中，会让决策树生长完全而不进行剪枝。构建出的若干棵决策树则组成了最终的随机森林。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;随机森林在众多分类算法中表现十分出众，其主要的优点包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;由于随机森林引入了样本扰动和特征扰动，从而很大程度上提高了模型的泛化能力，尽可能地避免了过拟合现象的出现。&lt;/li&gt;
&lt;li&gt;随机森林可以处理高维数据，无需进行特征选择，在训练过程中可以得出不同特征对模型的重要性程度。&lt;/li&gt;
&lt;li&gt;随机森林的每个基分类器采用决策树，方法简单且容易实现。同时每个基分类器之间没有相互依赖关系，整个算法易并行化。&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Q: 可否将随机森林中的基分类器，由决策树替换为线性分类器或KNN,为什么?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;随机森林属于Bagging类的集成学习。Bagging的主要好处是集成后的分类器的方差，比基分类器的方差小。Bagging所采用的基分类器，最好是本身对样本分布较为敏感的(即所谓不稳定的分类器)，这样Bagging才能有用武之地。线性分类器和KNN都是较为稳定的分类器，本身方差就不大，所以以它们为基分类器使用Bagging并不能在原有基分类器的基础上获得更好的表现，甚至可能因为Bagging的采样，而导致他们在训练中更难收敛，从而增大了集成分类器的偏差。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;boosting&#34;&gt;Boosting&lt;/h2&gt;
&lt;p&gt;Boosting 是一种提升算法，可以将弱的学习算法提升 (boost) 为强的学习算法。基本思路如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;利用初始训练样本集训练得到一个基学习器;&lt;/li&gt;
&lt;li&gt;提高被基学习器误分的样本的权重，使得那些被错误分类的样本在下一轮训练中可以得到更大的关注，利用调整后的样本训练得到下一个基学习器;&lt;/li&gt;
&lt;li&gt;重复上述步骤，直至得到 M 个学习器;&lt;/li&gt;
&lt;li&gt;对于分类问题，采用有权重的投票方式；对于回归问题，采用加权平均得到预测值。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;adaboost&#34;&gt;Adaboost&lt;/h3&gt;
&lt;p&gt;新预测器对其前序进行纠正的方法之一就是更多地关注前序欠拟合的训练实例，从而使新的预测器不断地越来越专注于难缠的问题。
例如，当训练AdaBoost分类器时，该算法首先训练一个基础分类器（例如决策树），并使用它对训练集进行预测。
然后，该算法会增加分类错误的训练实例的相对权重。然后，它使用更新后的权重训练第二个分类器，并再次对训练集进行预测，更新实例权重，以此类推。&lt;/p&gt;
&lt;h3 id=&#34;gbdt&#34;&gt;GBDT&lt;/h3&gt;
&lt;p&gt;GBDT (Gradient Boosting Decision Tree) 是另一种基于 Boosting 思想的集成算法。
GBDT 中使用的决策树不是分类树，而是回归树。回归树主要用于处理响应变量为数值型的数据，例如商品的价格。
GBDT 中在应用 Boost 概念时，每一轮所使用的数据集没有经过重抽样，也没有更新样本的权重，而是每一轮选择了不用的回归目标，
即上一轮计算得到的残差 (Residual)。其次，Gradient 是指在新一轮中在残差减少的梯度 (Gradient) 上建立新的基学习器。&lt;/p&gt;
&lt;p&gt;GBDT 中也应用到了 Shrinkage 的思想，其基本思想可以理解为每一轮利用残差学习得到的回归树仅学习到了一部分知识，
因此我们无法完全信任一棵树的结果。Shrinkage 思想认为在新的一轮学习中，不能利用全部残差训练模型，而是仅利用其中一部分，
Shrinkage 设置小一些可以避免发生过拟合现象；而 Gradient 中的步长如果设置太小则会陷入局部最优，如果设置过大又容易结果不收敛。&lt;/p&gt;
&lt;h3 id=&#34;xgboost&#34;&gt;XGBoost&lt;/h3&gt;
&lt;p&gt;XGBoost是陈天奇等人开发的一个开源机器学习项目。原始的GBDT算法基于经验损失函数的负梯度来构造新的决策树，只是在决策
树构建完成后再进行剪枝。而XGBoost在决策树构建阶段就加入了正则项。&lt;/p&gt;
&lt;p&gt;XGBoost 也采用了 Shrinkage 的思想减少每棵树的影响，为后续树模型留下更多的改进空间。
同时 XGBoost 也采用了随机森林中的特征下采样 (列采样) 方法用于避免过拟合，同时 XGBoost 也支持样本下采样 (行采样)。
XGBoost 在分裂点的查找上也进行了优化，使之能够处理无法将全部数据读入内存的情况，同时能够更好的应对一些由于数据缺失，大量零值和 One-Hot 编码导致的特征稀疏问题。
除此之外，XGBoost 在系统实现，包括：并行化，Cache-Aware 加速和数据的核外计算 (Out-of-Core Computation) 等方面也进行了大量优化。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Q: XGBoost与GBDT的联系和区别有哪些?&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;GBDT是机器学习算法，XGBoost是该算法的工程实现。&lt;/li&gt;
&lt;li&gt;在使用CART作为基分类器时，XGBoost显式地加入了正则项来控制模型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。&lt;/li&gt;
&lt;li&gt;GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。&lt;/li&gt;
&lt;li&gt;传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类器，比如线性分类器。&lt;/li&gt;
&lt;li&gt;传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机森林相似的策略，支持对数据进行采样。&lt;/li&gt;
&lt;li&gt;传统的GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺失值的处理策略。&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;lightgbm&#34;&gt;LightGBM&lt;/h3&gt;
&lt;p&gt;LightGBM 是由微软研究院的 Ke 等人提出了一种梯度提升树模型框架。之前的 GBDT 模型在查找最优分裂点时需要扫描所有的样本计算信息增益，
因此其计算复杂度与样本的数量和特征的数量成正比，这使得在处理大数据量的问题时非常耗时。LightGBM 针对这个问题提出了两个算法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Gradient-based One-Side Sampling (GOSS)&lt;/li&gt;
&lt;li&gt;Exclusive Feature Bundling (EFB)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;stacking&#34;&gt;Stacking&lt;/h2&gt;
&lt;p&gt;Stacking又称为 Stacked Generalization，是一种基于分层模型组合的集成算法，同时也是一种模型组合策略。Stacking 算法的基本思想如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;利用初级学习算法对原始数据集进行学习，同时生成一个新的数据集。&lt;/li&gt;
&lt;li&gt;根据从初级学习算法生成的新数据集，利用次级学习算法学习并得到最终的输出。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;对于初级学习器，可以是相同类型也可以是不同类型的。在新的数据集中，初级学习器的输出被用作次级学习器的输入特征，初始样本的标记仍被用作次级学习器学习样本的标记。&lt;/p&gt;
&lt;p&gt;一些模型组合策略：平均法和投票法。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;对于数值型的输出&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;简单平均法 (Simple Averaging)&lt;/li&gt;
&lt;li&gt;加权平均法 (Weighted Averaging)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对于分类任务&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;绝对多数投票法 (Majority Voting)&lt;/li&gt;
&lt;li&gt;相对多数投票法 (Plurality Voting)&lt;/li&gt;
&lt;li&gt;加权投票法 （Weighted Voting)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>机器学习笔记(二）- 模型评估和超参数优化</title>
        <link>https://sandyness.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%BA%8C-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E5%92%8C%E8%B6%85%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96/</link>
        <pubDate>Thu, 02 Jan 2020 00:00:00 +0000</pubDate>
        
        <guid>https://sandyness.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%BA%8C-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E5%92%8C%E8%B6%85%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96/</guid>
        <description>&lt;h2 id=&#34;模型性能评估&#34;&gt;模型性能评估&lt;/h2&gt;
&lt;p&gt;对模型的泛化性能进⾏评估，不仅需要有效可⾏的实验评估⽅法，还需要有衡量模型泛化能⼒的评价标准，也就是性
能度量（performance measure）。性能度量反映了任务需求，在对⽐不同模型的能⼒时，使⽤不同的性能度量往往会导
致不同的评判结果，这意味着模型的“好坏”是相对的，什么样的模型时好的，不仅仅取决于算法和数据，还决定于任务需求。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;回归问题&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;平均绝对误差（mean absolute error, MAE）&lt;/li&gt;
&lt;li&gt;平均绝对百分⽐误差（mean absolute percentage error, MAPE）&lt;/li&gt;
&lt;li&gt;均⽅误差（mean squared error, MSE）&lt;/li&gt;
&lt;li&gt;均⽅根误差（root-mean-square error, RMSE）&lt;/li&gt;
&lt;li&gt;误差标准差&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;分类问题&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;误差和精度&lt;/li&gt;
&lt;li&gt;准确率，召回率和 F1 Score&lt;/li&gt;
&lt;li&gt;ROC 和 AUC&lt;/li&gt;
&lt;li&gt;多分类 Log Loss&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;聚类问题&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;外部指标&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Jaccard 系数（Jaccard Coefficient，JC）&lt;/li&gt;
&lt;li&gt;FM 指数（Fowlkes and Mallows Index，FMI）&lt;/li&gt;
&lt;li&gt;Rand 指数（Rand Index，RI）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;内部指标：考虑聚类结果的簇划分 ，定义：C = {C1,C2,&amp;hellip;,Ck}&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;簇C内样本间的平均距离&lt;/li&gt;
&lt;li&gt;簇C内样本间最远距离&lt;/li&gt;
&lt;li&gt;簇Ci与Cj最近样本间的距离&lt;/li&gt;
&lt;li&gt;簇Ci与Cj中⼼点间的距离&lt;/li&gt;
&lt;li&gt;DB 指数（Davies-Bouldin Index，DBI）类⽐簇间相似度&lt;/li&gt;
&lt;li&gt;Dunn 指数（Dunn Index，DI）类⽐簇内相似度
DBI 的值越小越好，DI 相反，越⼤越好。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;模型生成和选择&#34;&gt;模型生成和选择&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;过拟合问题&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;L1正则化&lt;/li&gt;
&lt;li&gt;L2正则化&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;评估方法&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;留出法&lt;/li&gt;
&lt;li&gt;交叉验证&lt;/li&gt;
&lt;li&gt;⾃助法&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;偏差和方差&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;泛化误差可以分解为偏差、⽅差与噪声之和。&lt;/p&gt;
&lt;p&gt;偏差：度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本⾝的拟合能⼒。&lt;/p&gt;
&lt;p&gt;⽅差：度量了同样⼤小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响。&lt;/p&gt;
&lt;p&gt;噪声：则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本⾝的难度。&lt;/p&gt;
&lt;p&gt;偏差-⽅差分解说明，泛化性能是由学习算法的能⼒、数据的充分性以及学习任务本⾝的难度所共同决定的。给定学习任务，为了取得好的泛化性能，需要使偏差较小，即能够充分拟合数据，且使⽅差较小，即使数据扰动产⽣的影响较小。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;超参数优化&#34;&gt;超参数优化&lt;/h2&gt;
&lt;p&gt;模型的参数和超参数⼆者有着本质上的区别：模型参数是模型内部的配置变量，可以⽤数据估计模型参数的值，例如：
回归中的权重，决策树分类点的阈值等；模型超参数是模型外部的配置，必须⼿动设置参数的值，例如：随机森林树的
个数，聚类⽅法⾥⾯类的个数，或者主题模型⾥⾯主题的个数等。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;搜索算法&lt;/strong&gt;: ⽹格搜索，随机搜索等&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Grid Search 是⼀个暴⼒解法，通过所有需要测试的超参数，找出所有可能的超参数组合，最根据验证集的损失找出最好的⼀组超参数。这是⼀个⾮常消耗资源的⽅
法。如果有个超参数，每个超参数选最多个可能值，那么该算法的时间复杂度是 。&lt;/li&gt;
&lt;li&gt;Random Search 的使⽤⽅法 Grid Search 基本⼀致，区别在于 Random Search 会在超参数的组合空间内随机采样搜索，其搜索能⼒取决于设定的抽样次数，最重要的是收敛更快。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;启发式算法&lt;/strong&gt;: 遗传算法，粒⼦群算法等&lt;/p&gt;
&lt;p&gt;启发式算法（Heuristic Algorithms）是相对于最优算法提出的。
⼀个问题的最优算法是指求得该问题每个实例的最优解。
启发式算法可以这样定义：⼀个基于直观或经验构造的算法，在可接受的花费（指计算时间、占⽤空间等）下给出待解决组合优化问题每⼀个实例的⼀个可⾏解，该可⾏解与最优解的偏离程度不⼀定事先可以预计。
在某些情况下，特别是实际问题中，最优算法的计算时间使⼈⽆法忍受或因问题的难度使其计算时间随问题规模的增加以指数速度增加，此时只能通过启发式算法求得问题的⼀个可⾏解。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;贝叶斯优化&lt;/strong&gt;: ⾼斯过程，TPE 等&lt;/p&gt;
&lt;p&gt;Grid Search 和 Randomized Search 可以让整个调参过程⾃动化，但它们⽆法从之前的调参结果中获取信息，可能会尝试很多⽆效的参数空间。
而⻉叶斯优化，会对上⼀次的评估结果进⾏追踪，建⽴⼀个概率模型，反应超参数在⽬标函数上表现的概率分布⽤于指导下⼀次的参数选择。
⻉叶斯优化适⽤于随机、⾮凸、不连续⽅程的优化。Sequential Model-Based Optimization (SMBO) 是⻉叶斯优化更具体的表现形式，⼀般会有以下⼏个过程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;给定要搜索的超参数空间&lt;/li&gt;
&lt;li&gt;定义⼀个⽬标函数⽤于评估优化&lt;/li&gt;
&lt;li&gt;建⽴⽬标函数的 Surrogate Model&lt;/li&gt;
&lt;li&gt;建⽴⼀个选择超参数的标准的评估 Surrogate Model&lt;/li&gt;
&lt;li&gt;获取评分和超参数的样本⽤于更新 Surrogate Model&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>机器学习笔记(一) - 特征工程</title>
        <link>https://sandyness.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/</link>
        <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
        
        <guid>https://sandyness.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/</guid>
        <description>&lt;p&gt;特征工程，是对原始数据进行一系列的工程处理，去除原始数据的杂质和冗余，将其提炼为可供机器学习的输入特征。机器学习常用的数据类型包括结构化数据和非结构化数据。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;结构化数据&lt;/strong&gt;：可以看作关系型数据库的一张表，每列都有清晰的定义，包含了数值型、类别型两种基本类型;每一行数据表示一个样本的信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;非结构化数据&lt;/strong&gt;：主要包括文本、图像、音频、视频数据，其包含的信息无法用一个简单的数值表示，也没有清晰的类别定义，并且每条数据的大小各不相同。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;数据预处理&#34;&gt;数据预处理&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;处理缺失数据&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;删除缺失值：pandas 中使用 dropna()函数&lt;/li&gt;
&lt;li&gt;补全缺失值
&lt;ul&gt;
&lt;li&gt;pandas 中使用 fillna()函数&lt;/li&gt;
&lt;li&gt;单变量补全
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.impute&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SimpleImputer&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;多变量补全
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.impute&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;IterativeImputer&lt;/span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;KNN补全
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.impute&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;KNNImputer&lt;/span&gt;       
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;删除重复数据&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;特征去重: 对于仅相差常数倍的特征需要进⾏去重处理&lt;/li&gt;
&lt;li&gt;常量特征剔除: 对于常量或⽅差近似为零的特征，其对于样本之间的区分度贡献为零或近似为零&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;异常值检测&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;异常值&lt;/strong&gt;：是指样本中存在的同样本整体差异较⼤的数据，异常数据可以划分为两类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;异常值不属于该总体，而是从另⼀个总体错误抽样到样本中而导致的较⼤差异。&lt;/li&gt;
&lt;li&gt;异常值属于该总体，是由于总体所固有的变异性而导致的较⼤差异。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于数值型的单变量，可以利⽤拉依达准则对其异常值进⾏检测。
假设总体 x 服从正态分布，则： P (|x − μ| &amp;gt; 3σ) ≤ 0.003 其中 μ 表⽰总体的期望，
σ 表⽰总体的标准差。因此，对于样本中出现⼤于 μ + 3σ 或小于 μ − 3σ 的数据的概率是⾮常小的，从而可以对⼤于 μ + 3σ 和小于 μ − 3σ 的数据予以剔除。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;异常检测（Anomaly Detection)&lt;/strong&gt; ：是指对不符合预期模式或数据集中异常项⽬、事件或观测值的识别。
通常异常的样本可能会导致银⾏欺诈、结构缺陷、医疗问题、⽂本错误等不同类型的问题。异常也被称为离群值、噪声、偏差和例外。
常⽤的异常检测算法有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于密度的⽅法：最近邻居法、局部异常因⼦等&lt;/li&gt;
&lt;li&gt;One-Class SVM&lt;/li&gt;
&lt;li&gt;基于聚类的⽅法&lt;/li&gt;
&lt;li&gt;Isolation Forest&lt;/li&gt;
&lt;li&gt;AutoEncoder&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;特征变换和编码&#34;&gt;特征变换和编码&lt;/h2&gt;
&lt;h3 id=&#34;数值类型&#34;&gt;数值类型&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;特征归一化&lt;/strong&gt;: 为了消除数据特征之间的量纲影响，需要对特征进行归一化(Normalization)处理，将所有的特征都统一到一个大致相同的数值区间内，在随机梯度下降的过程中，
特征的更新速度变得更为一致，容易更快地通过梯度下降找到最优解。&lt;/p&gt;
&lt;p&gt;最常用的方法主要有以下两种:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;线性函数归一化(Min-Max Scaling)&lt;/strong&gt;：对原始数据进行线性变换，使结果映射到[0, 1]的范围，实现对原始数据的等比缩放。公式如下：
X= (X - Xmax) / (Xmax - Xmin)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;MinMaxScaler&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;其中X为原始数据，Xmax、Xmin分别为数据最大值和最小值。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;零均值归一化(Z-Score Normalization)&lt;/strong&gt;：它会将原始数据映射到均值为0、标准差为1的分布上。若原始特征的均值为μ、标准差为σ,归一化公式定义为：
z = (X - μ) / σ&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;StandardScaler&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在实际应用中，通过梯度下降法求解的模型通常是需要归一化的，包括&lt;strong&gt;线性回归&lt;/strong&gt;、&lt;strong&gt;逻辑回归&lt;/strong&gt;、&lt;strong&gt;支持向量机&lt;/strong&gt;、&lt;strong&gt;神经网络&lt;/strong&gt;等模型。
但对于&lt;strong&gt;决策树&lt;/strong&gt;模型则并不适用，决策树在进行节点分裂时主要依据数据集和特征的信息增益比，而信息增益比跟特征是否经过归一化是无关的，因为归一化并不会改变样本在特征上的信息增益。&lt;/p&gt;
&lt;h3 id=&#34;类别类型&#34;&gt;类别类型&lt;/h3&gt;
&lt;p&gt;类别型特征(Categorical Feature)是只在有限选项内取值的特征，如性别(男、女)、血型(A、B、 AB、O)等。&lt;/p&gt;
&lt;p&gt;类别型特征原始输入通常是字符串形式，除了决策树等少数模型能直接处理字符串形式的输入，对于逻辑回归、支持向量机等模型来说，类别型特征必须经过处理转换成数值型特征才能正确工作。&lt;/p&gt;
&lt;p&gt;最常用的方法主要有以下两种。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;序号编码（Ordinal Encoding）&lt;/strong&gt;: 常用来处理具有大小关系的类别特征（eg.成绩高中低）&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;OrdinalEncoder&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;独热编码(One-hot Encoding）&lt;/strong&gt;：不具有大小关系的类别特征（eg.血型）&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;OneHotEncoder&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;在独热编码下，特征向量只有某一维取值为1，其他位置取值均为0。因此可以利用向量的稀疏表示有效地节省空间，并且目前大部分的算法均接受稀疏向量形式的输入。&lt;/li&gt;
&lt;li&gt;高维度特征会带来几方面的问题。一是在K近邻算法中，高维空间下两点之间的距离很难得到有效的衡量;二是在逻辑回归模型中，参数的数量会随着维度的增高而增加，容易引起过拟合问题;三是通常只有部分维度是对分类、预测有帮助，因此可以考虑配合特征选择来降低维度。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;二进制编码(Binary Encoding)&lt;/strong&gt;：利用二进制对ID进行哈希映射，最终得到0/1特征向量，且维数少于独热编码，节省了存储空间。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;特征提取和选择&#34;&gt;特征提取和选择&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;特征选择&lt;/strong&gt;是从⼀组特征中选出⼀些最有效的特征，使构造出来的模型更好。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;避免过度拟合，改进预测性能&lt;/li&gt;
&lt;li&gt;使学习器运⾏更快，效能更⾼&lt;/li&gt;
&lt;li&gt;剔除不相关的特征使模型更为简单,容易解释&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;过滤⽅法（Filter Methods）&lt;/strong&gt;：按照发散性或相关性对特征进⾏评分，设定阈值或者待选择阈值的个数，选择特征。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;⽅差选择法：选择⽅差⼤的特征。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;相关关系 &amp;amp; 卡⽅检验：特征与⽬标值的相关关系。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;互信息法：⼀个随机变量包含另⼀个随机变量的信息量。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;封装⽅法（Wrapper Methods）：是利⽤学习算法的性能来评价特征⼦集的优劣。因此，对于⼀个待评价的特征⼦集， Wrapper⽅法需要训练⼀个分类器，根据分类器的性能对该特征⼦集进⾏评价，学习算法包括决策树、神经⽹络、⻉叶斯分类器、近邻法以及⽀持向量机等。Wrapper⽅法缺点主要是特征通⽤性不强，当改变学习算法时，需要针对该学习算法重新进⾏特征选择。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;集成⽅法（Embedded Methods）：在集成法特征选择中，特征选择算法本⾝作为组成部分嵌⼊到学习算法⾥。最典型的是决策树算法。包括基于惩罚项的特征选择法和基于树模型的特征选择法。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# 方差选择法&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.feature_selection&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;VarianceThreshold&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# 根据 k 个最高分选择特征&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.feature_selection&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SelectKBest&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# 卡方检验&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.feature_selection&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chi2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;# 基于重要性权重选择特征&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.feature_selection&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SelectFromModel&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;特征监控&#34;&gt;特征监控&lt;/h2&gt;
&lt;p&gt;在数据分析和挖掘中，特征占据着很重要的地位。
因此，我们需要对重要的特征进⾏监控与有效性分析，了解模型所⽤的特征是否存在问题，当某个特别重要的特征出问题时，需要做好备案，防⽌灾难性结果。&lt;/p&gt;
&lt;h2 id=&#34;参考资料&#34;&gt;参考资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://scikit-learn.org/stable/modules/classes.html?highlight=feature_selection#module-sklearn.feature_selection&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;sklearn.feature_selection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;sklearn.preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://scikit-learn.org/stable/modules/classes.html?highlight=sklearn&amp;#43;impute#module-sklearn.impute&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;sklearn.impute&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
